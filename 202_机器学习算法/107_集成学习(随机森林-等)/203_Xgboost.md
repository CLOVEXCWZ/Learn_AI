# XGBoost

[博客网址============](https://www.jianshu.com/p/8c62fb63fc1b)

## 一 步骤整理

### 1 第一步：整体误差(重点：整体视角)

 整体误差指的是XGBoost模型训练完成之后，将训练集中所有实例代入模型，用以下函数（总误差L()）衡量模型的好坏：
$$
L(Φ)=\sum_{i}l(\widehat{y},y_i)+\sum_{k}Ω(f_k)
$$
 左边是训练集所有实例的误差之和，i指每个实例，y^是预测值，y是实际值，而l()是衡量y’与y差异的方法，比如RMSE。左边比较好理解，就是说训练一个模型，最好能对于所有的实例都做出与真实值相似的预测。

 右边是正则项，它的用途就是防止模型过拟合，比如说一个模型，一共400个实例，模型做了400个叶节点，与实例一一对应，它的泛化就很差，所以应该尽量简化模型，正则项在第四步详述。

### 2 第二步：计算t颗树时的误差(重点:从第t-1颗树到第t颗树)

​	梯度下降决策树是由多棵树组成的模型。假设它由t棵树组成，误差是：
$$
L^{(t)}=\sum_{i=1}^{n}(y_i,\widehat{y}^{(t-1)}+f_t(x_i))+Ω(f_t)
$$
 还是看左边，计算n个实例误差的总合，y是实际值，而此时的预测值是，之前树的预测值y’(t-1)，加上第t棵树的预测值ft()，ft()就是第t棵树所做的工作。

### 3 泰勒公式（重点：从始至终计算都是预测误差L()）

 泰勒公式：在已知函数在某一点（x0点）的各阶导数值的情况之下，泰勒公式可以用这些导数值做系数构建一个多项式来近似函数在这一点的邻域中的值。
$$
f(x)=\frac{f(x_0)}{0!}+\frac{f'(x_0)}{1!}(x-x_0)+\frac{f''(x_0)}{2!}(x-x_0)^2+\\
...+\frac{f^n(x_0)}{n!}(x-x_0)^n+R_n(x)
$$
其中Rn(x)是余项。

 简单举个例子，我不知道x住哪儿f(x)，但知道x附近的x0住哪儿f(x0)，所以我先找到它，然后根据他俩距离的远近x-x0，以及他俩位置的相对方向（f()的导数），推出x大概住哪儿。

 换种写法，求点x附近，距离是delta x的点的f()，只考虑两阶导数：
$$
f(x+Δx)=f(x)+\frac{f'(x)}{1}Δx+\frac{f''(x)}{2}Δx^2
$$
 本文中所求的函数f()是误差函数L()，代入进去是：
$$
L(x+Δx)=L(x)+\frac{L'(x)}{1}Δx+\frac{L''(x)}{2}Δx^2
$$
这里的delta x指的是x的细微变化，回想第二步的公式中，我们每训练一棵树ft()都相当于对上一步结果的微调，于是有：
$$
L(x+f_t)=L(x)+\frac{L'(x)}{1}f_t+\frac{L''(x)}{2}f_t^2
$$
 也就是说，知道第t-1棵树预测结果与真实值的误差L(x)，这个误差函数的一阶导L’(x)，二阶导L’’(x)，又知道第t棵相对于第t-1棵做了什么微调ft()，于是估计出：加入第t棵树之后的预测值与真实值的误差。就有了以下公式：
$$
L^{(t)}=\sum_{i=1}^{n}(y_i,\widehat{y}^{(t-1)}+f_t(x_i))+Ω(f_t)\\
=\sum_{i=1}^{n}(\widehat{y}^{(t-1)} + g_1f_t(x_i)+\frac{1}{2}h_if_t^2(x_i))+Ω(f_t)\\
一阶导数：g_i=\widehat{y}^{(t-1) ’}   \\
二阶导数：h_i=\widehat{y}^{(t-1) ‘’}  \\
$$
 其中：gi和hi分别是误差函数l()对第t-1棵预测值的一阶导和二阶导。
简单地说：一共t棵树的模型，它的误差是第t-1棵树构造模型的误差函数l()，加上误差函数一阶导gi乘第t棵树的贡献ft()，再加误差函数的二阶导hi乘第t棵树贡献的平方。

### 4 公式右侧正则项（重点：得分w）

 先来看看最普通的单棵决策树，当训练完模型后，预测时把x代入树，顺着条件判断的分支，最后落入哪个叶节点，结果就是该叶节点的值。
 而Boost决策树是多棵决策树，它对x的预测结果是，将x代入每棵决策树，得到多个叶节点的值w，将其结果累加得到预测值（最基本的逻辑）。这里各个叶节点的值w简称得分。
正则项是为了防止模型太复杂过拟合，公式如下：
$$
Ω(f)=γT+\frac{1}{2}γ\sum_{j=1}^{T}w_j^2
$$
T:叶子节点个数

w:是叶节点得分

y和λ 是可调节参数

 其中T是树中的叶结点个数，w是叶节点的得分，γ和λ是可调节的参数，在整体L()计算公式中，w越大误差L越大(w不均匀)，树的叶子越多L也越大，为求得最小的L，最终在树的复杂度和准确度之间取得平衡。

### 5 以实例为单位变为以节点为单位累加（重点：转换视角）

 此时我们的焦点在ft(x)上，要分析L与ft的关系，第t-1棵树误差是个常数项，先忽略不看，可写成:
$$
L^{(t)}=\sum_{i=1}^{n}(g_1f_t(x_i)+\frac{1}{2}h_if_t^2(x_i))+Ω(f_t)
$$
 每一个xi是一个实例的条件，它经过第t棵决策树ft()的处理后，会落在某个叶节点上，得到该叶节点的得分w，即：ft(xi)->wj，因此可将ft(xi)转换为wj，代入，得到以下公式：
$$
L^{(t)}=\sum_{i=1}^{n}((\sum_{i=I_j}g_i)w_j+\frac{1}{2}(\sum_{i=I_j}h_i)w_j^2)+Ω(f_t)
$$
 其中T是树的叶节点个数，需要注意的是其中Ij，它指的是落入树中节点j的所有训练实例。把右侧展开后加入，得到：
$$
L^{(t)}=\sum_{i=1}^{n}((\sum_{i=I_j}g_i)w_j+\frac{1}{2}(\sum_{i=I_j}h_i)w_j^2)+Ω(f_t)\\
Ω(f)=γT+\frac{1}{2}γ\sum_{j=1}^{T}w_j^2\\
L^{(t)}=\sum_{i=1}^{n}((\sum_{i=I_j}g_i)w_j+\frac{1}{2}(\sum_{i=I_j}h_i+γ)w_j^2)+γT\\
$$


### w如何取值使预测误差最小（重点：求极值）

 这是个极值问题，求当误差函数L()为最小值时，wj的取值，求极值即导数为0的点，简单推导如下（写得不全，领会精神）：
$$
0=(gw+\frac{1}{2}(h+γ)w^2+c)’\\
0=g+(h+γ)w\\
w=-\frac{g}{h+γ}
$$
规范的写法是：
$$
L^{(t)}=-\frac{1}{2}\sum_{j=1}^{T}\frac{(\sum_{u=I_j}g_i)^2}{\sum_{j=I_j}h_i+γ}+γT
$$
 用语言描述公式：误差最小的条件是，对所有（T个）叶节点，代入落入该节点的实例，用之前t-1棵树的误差的导出和正则项，即可计算出第t棵树的误差。
此处我们看到，计算第t棵树的误差时，不需要计算出该树每个叶节点的w，只需把计算w的素材h,g,Ij,λ代入即可。

### 7 在分裂决策树时计算误差函数（重点：细化到每一次分裂）

 此步骤关注的不是整棵树，而是每次分裂，最基本的贪婪算法是：生成树时，从根节点开始，遍历所有属性，遍历所有属性的可能取值作为分裂点，计算该分裂点左子树样本集合ll和右子树样本集合lr的误差，加起来和不分裂的误差相比，即可判断分裂是否合理。
注意这时的误差L计算的不是全树的误差，而权限于与本次分裂相关的实例在分裂前后的误差对比。 
$$
Gain=\frac{1}{2}[\frac{G_L^2}{H_L+λ}+\frac{G_R^2}{H_R+λ}-\frac{G_L+G_R^2}{H_L+H_R+λ}]-γ
$$

「这里没有颜色哦」

红色式子表示：划分后左叶子结点的分值
蓝色式子表示：划分后右叶子结点的分值
绿色式子表示：划分前该结点的分值
粉色式子表示：将该结点划分为叶子结点的复杂度代价



### 优缺点

- 优点
  - 可以并行
  - 在特征选择的时候就进行了正则化约束
  - 能够自动处理缺失值
  - 