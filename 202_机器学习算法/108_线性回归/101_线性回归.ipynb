{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概念\n",
    "    线性回归（Linear Regression）是一种通过属性的线性组合来进行预测的线性模型，其目的是找到一条直线或者一个平面或者更高维的超平面，使得预测值与真实值之间的误差最小化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推导过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求解目标函数\n",
    "\n",
    "![101](picture/101.png)\n",
    "    \n",
    "    预测值和真实值之间的差异 ，对每个样本：\n",
    "![102](picture/102.png)  \n",
    " \n",
    "    假设误差是独立同分布的，并且服从高斯分布。即：\n",
    "![103](picture/103.png)\n",
    "\n",
    "    将（2）代入（3）中，得到在一直参数w和数据wi的情况下，预测值为yi的条件概率：\n",
    "![104](picture/104.png)\n",
    "\n",
    "    将（4）连乘得到在已知参数w和数据x的情况下，预测值为y的条件概率，这个条件概率在数值上等于，likelihood（w|x,y），也就是在已知现有数据的条件下，w是真正参数的概率，即似然函数（5）：\n",
    "![105](picture/105.png)   \n",
    "\n",
    "    __ 为什么要引入依然函数：为了根据样本估计参数值__\n",
    "    ___我什么要对似然函数进行log变换：由于乘法难解，可以将乘法转化为加法，简化计算__\n",
    "   \n",
    "   \n",
    "    对数似然函数：\n",
    "![106](picture/106.png)   \n",
    "\n",
    "    得到目标函数：\n",
    "![107](picture/107.png)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降 - 求偏导数\n",
    "\n",
    "    损失函数\n",
    "![108](picture/108.png)\n",
    "\n",
    "    展开式\n",
    "![109](picture/109.png)\n",
    "\n",
    "    几个矩阵求导公式\n",
    "![110](picture/110.png)\n",
    "\n",
    "    对于展开式求偏导\n",
    "![1111](picture/111.png)\n",
    "![1112](picture/112.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 风险函数\n",
    "\n",
    "## 期望风险\n",
    "    期望风险（expected risk）:\n",
    "    描述模型与训练样本以及测试样本的拟合程度。对所有样本（包含已知样本和未知样本）的预测能力，是全局概念，是理性化的不可求的。\n",
    "\n",
    "## 经验风险\n",
    "    经验风险（empirical risk）:\n",
    "    描述模型与训练样本的拟合程度。对所有训练样本的预测能力，是局部概念，是显示可求的 \n",
    "     (例如 损失函数)\n",
    "\n",
    "## 结构风险\n",
    "    结构风险（structural risk）:\n",
    "    描述模型与训练样本的拟合程度，以及模型的复杂程度。对过拟合线性的处理\n",
    "    （例如 损失函数 加入正则项）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对损失函数加入 正则项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1范数\n",
    "    描述向量中各元素的平方之和，然后求平方根。\n",
    "    L2范数会选择更多的特征，这些特征都会趋近于0。\n",
    "\n",
    "\n",
    "    𝐿1 范数为什么能够实现模型参数向量的稀疏呢?\n",
    "![113](picture/113.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 范数\n",
    "    描述向量中各元素的平方之和，然后求平方根。\n",
    "    L2范数会选择更多的特征，这些特征都会趋近于0。\n",
    "    \n",
    "    \n",
    "    𝐿2 范数不能实现模型参数向量的稀疏?\n",
    "![113](picture/113.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips\n",
    "    如果模型的特征非常多，我们希望一些不重要特征的系数归零， 从而让模型的系数稀疏化，那么选择L1正则化。\n",
    "    如果我们需要相对精确的多元逻辑回归模型，那么L1正则化可能就不合适了。\n",
    "    在调参时，如果我们目的仅仅是为了解决过拟合问题，一般选择L2正则化就可以; 但是，如果选择L2正则化之后，发现还是存在过拟合的问题，就可以考虑L1正则化。\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优缺点\n",
    "\n",
    "## 优点\n",
    "- 实现简单，计算简答\n",
    "- 可解释性强\n",
    "\n",
    "## 缺点\n",
    "- 不能拟合非线性数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
