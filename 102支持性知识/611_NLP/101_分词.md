# 中文分词

## 常用的中文分词方法

### 基于字符串匹配的分词方法

- 正向最大匹配FMM
  - 从左到右将待分词文本中的最多个连续字符与词表匹配，如果匹配上，则切分出一个词。
- 逆向最大匹配
  - 从右到左将待分词文本中的最多个连续字符与词表匹配，如果匹配上，则切分出一个词。
- 双向最大匹配
  - 正向最大匹配算法和逆向最大匹配算法．如果两个算法得到相同的分词结果，那就认为是切分成功，否则，就出现了歧义现象或者是未登录词问题。
- N-gram双向最大匹配
  - 基于字符串的分词方法中的正向最大匹配算法和逆向最大匹配算法。然后对两个方向匹配得出的序列结果中不同的部分运用Bi-gram计算得出较大概率的部分。最后拼接得到最佳词序列。
- HMM分词



### 基于理解的分词方法

**基于理解的分词方法**是**通过让计算机模拟人对句子的理解**，达到识别词的效果。其**基本思想**就是**在分词的同时进行句法、语义分析**，利用**句法信息**和**语义信息**来**处理歧义**现象。它通常包括三个部分：**分词子系统**、**句法语义子系统**、**总控部分**。在**总控部分**的协调下，**分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断**，即它模拟了人对句子的理解过程。这种分词方法**需要使用大量的语言知识和信息**。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式，因此目前基于理解的分词系统还处在**试验阶段**。



### 基于统计的分词方法

**基于统计的分词方法**是在**给定大量已经分词的文本**的前提下，**利用统计机器学习模型学习词语切分的规律**（称为**训练**），从而实现**对未知文本的切分**。例如**最大概率分词方法**和**最大熵分词方法**等。随着大规模语料库的建立，统计机器学习方法的研究和发展，**基于统计的中文分词方法渐渐成为了主流方法**

主要的**统计模型**有：**N元文法模型**（N-gram），**隐马尔可夫模型**（Hidden Markov Model ，HMM），**最大熵模型**（ME），**条件随机场模型**（Conditional Random Fields，CRF）等。

在实际的应用中，**基于统计的分词系统**都需要使用**分词词典**来**进行字符串匹配分词**，同时**使用统计方法识别一些新词**，即将**字符串频率统计**和**字符串匹配**结合起来，既发挥**匹配分词切分速度快、效率高的特点**，又利用了**无词典分词结合上下文识别生词、自动消除歧义的优点**。





- 序列标注

B：起始词

M：中间词

E：结束词

S：单独的词

比如：

['现在', '县', '财政', '的', '近', '９０％', '来自', '乡镇企业', '，']

['B', 'E', 'S', 'B', 'E', 'S', 'S', 'B', 'M', 'E', 'B', 'E', 'B', 'M', 'M', 'E', 'S']

 

