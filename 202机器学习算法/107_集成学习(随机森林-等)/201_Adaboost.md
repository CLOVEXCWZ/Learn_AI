# Adaboost算法原理分析

[博客地址=========](https://blog.csdn.net/guyuealian/article/details/70995333)

## 1 Adaboost简介

​	Boosting,也称为增强学习或提升法，是一种重要的集成学习技术， 能够将预测精度仅比随机猜度略高的弱学习器增强为预测精度高的强学习器，这在直接构造强学习器非常困难的情况下，为学习算法的设计提供了一种有效的新思路和新方法。其中最为成功应用的是，Yoav Freund和Robert Schapire在1995年提出的AdaBoost算法。

​	 AdaBoost是英文"Adaptive Boosting"（自适应增强）的缩写，它的自适应在于：前一个基本分类器被错误分类的样本的权值会增大，而正确分类的样本的权值会减小，并再次用来训练下一个基本分类器。同时，在每一轮迭代中，加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数才确定最终的强分类器。 

**Adaboost算法简述为三个步骤**

- 1 首先，是初始化训练数据的权值分布D1。假设有N个训练样本数据，则每一个训练样本最开始时，都被赋予相同的权值：w1=1/N

- 2 然后，训练弱分类器hi。具体训练过程是：如果某个训练样本点被弱分类器hi准确地分类，那么构造下一个训练集中，它对应的权值要减小；相反，如果某个训练样本点被错误分类，那么它的权值应该增大，权值更新过的样本就用于训练下一个分类器，整个训练过程如此迭代地进行下去

- 3 最后，将各个训练得到的弱分类器，组合成一个强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终分函数中起着较小的决定作用，

  换而言之，误差率低的弱分类器在最终的分类器中占权重比较多啊，否则较小

## 2 Adaboost算法过程

​	给定训练数据集：（x1,y1）,......(xn, yn), 其中 yi属于{1, -1}用于表示训练样本的类别标签，*i=1,...,N*。Adaboost的目的就是从训练数据中学习一系列弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。

**相关符号定义**
$$
D_i(i) : 训练样本集的权值分布\\
w_i:每个训练样本的权值大小\\
h:弱分类器\\
H:基本分类器\\
H_{final}：最终强分类器\\
e:误差率\\
a_i:弱分类器的权重
$$

- Adaboost的算法流程如下

  - 1 首先，初始化训练样本的权值分布。每一个训练样本最开始都被赋予相同的权值: w=1/N,这样迅游的集的初始值分布D1(i):

  $$
  D_i(i)=(w_1,w_2,...,w_N)=(\frac{1}{N},...,\frac{1}{N})
  $$

  - 2 进行迭代 t=1,....T

    - a 选取一个当前误差率最低的弱分类器h做低t个基本分类器Ht,并计算弱分类器h:X->{-1, 1}，该弱分类器在分布Dt上的误差

    $$
    e_t=P(H_t(x_i)≠y_i)=\sum_{i=1}^{N}w_{ti}I(H_i(x_i)≠y_i)
    $$

    - b 计算弱分类器最终所占权重

    $$
    a_t=\frac{1}{2}ln(\frac{1-e_t}{e_t})
    $$

    - c 更新训练样本分布D

    $$
    D_{t+1}=\frac{D_t(i)exp(-a_ty_iH_t(x_i))}{z_t}\\
    其中Z_t为归一化常数Z_t= 2\sqrt{e_t(1-e_t)}
    $$

    $$
  错误样本分类时:D_{t+1}(i)=\frac{D_t(i)}{2e_t}\\
    正确样本分类时:D_{t+1}(i)=\frac{D_t(i)}{2(1-e_t)}\\
  $$
  
    
  
  - 3 最后按照弱分类器权值a组合弱分类器
  
$$
  f(x)=\sum_{t=1}^{T}a_tH_t(x)\\
  通过符号sign的作用得到一个强分类器为：\\
  H_{final}=sign(f(x))=sign(\sum_{t=1}^{T}a_tH_t(x))
  $$
  
  



## Adaboost的优缺点

- 优点
  - 具有很高的精度
  - 简单，不用做特征选择
  - 不用担心overfitting
- 缺点

  - 在Adaboost训练过程中，**Adaboost会使得难于分类样本的权值呈指数增长**，训练将会过于偏向这类困难的样本，导致Adaboost算法易受噪声干扰。此外，Adaboost依赖于弱分类器，而弱分类器的训练时间往往很长。
- 容易受噪音影响
  - 训练比较耗时，每次重新选择当前分类器最好的切点
  
