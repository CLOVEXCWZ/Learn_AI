# XGBoost 实例

[博客原地址==================](https://blog.csdn.net/weixin_41580067/article/details/86220782)



上面的算法流程有些抽象，所以我们还是以实例来一步一步的实现XGBoost,数据集如下表：

| ID   | x1   | x2   | y    |
| ---- | ---- | ---- | ---- |
| 1    | 1    | -5   | 0    |
| 2    | 2    | 5    | 0    |
| 3    | 3    | -2   | 1    |
| 4    | 1    | 2    | 1    |
| 5    | 2    | 0    | 1    |
| 6    | 6    | -5   | 1    |
| 7    | 7    | 5    | 1    |
| 8    | 6    | -2   | 0    |
| 9    | 7    | 2    | 0    |
| 10   | 6    | 0    | 1    |

数据集中有10个样本，两个特征x1,x2
x1,x2，为了简单起见

我们预定义树的深度为1（max_depth=1）

树的颗数为2（num_boost_round=2）

,学校率为0.1（eta=0.1）

正则化参数λ=1,γ=0
λ=1,γ=0，损失函数（logloss）

我们选取的损失函数为：
$$
l(y_i,\widehat{y}_i)=y_iln(1+e^{-\widehat{y}_i})+(1-y_i)ln(1+e^{\widehat{y}_i})
$$
注意：这里的yiˆ  是没有映射为概率的原始值，即

$$
\widehat{y}_i=w^T*x_i+b_i
$$
那么映射后，预测值为1的概率为：
$$
P_{1i}=\frac{1}{1+e^{-y_i}}
$$
后面我们需要用到logloss的一阶、二阶导数，有必要先写出其导数：
$$
g_i=l'(y_i,\widehat{y}_i)=P_{1i}-y_i\\
h_i=l''(y_i,\widehat{y}_i)=P_{1i}(1-P_{1i})
$$
下面我们利用XGboost拟合数据集

## 生成第一棵树

回顾我们上面的原理分析，对该结点是否划分的准则是计算增益：
$$
Gini=\frac{1}{2}[\frac{G_L^2}{H_L+λ}+\frac{G_R^2}{H_R+λ}-\frac{(G_L+G_R)^2}{H_L+H_R+λ}]-γ
$$
那么在求GL,GR  时需要用到上一颗树的预测值，也就是我们在GBDT中一样的初始值，在XGBoost用base_score表示初始值，默认为base_score=0.5 

值得一提的是，base_score是一个经过映射后的值，可以理解为预测为1的概率值，因为在建立第二颗树时会用到，所以应该留意此处。

对每一个样本求出其一阶、二阶导数的值：

| ID   | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| gi   | 0.5  | 0.5  | -0.5 | -0.5 | -0.5 | -0.5 | -0.5 | 0.5  | 0.5  | -0.5 |
| hi   | 0.25 | 0.25 | 0.25 | 0.25 | 0.25 | 0.25 | 0.25 | 0.25 | 0.25 | 0.25 |

计算步骤如下：
对于ID=1的样本（其他样本计算类似）

g1=l′(y1,y1ˆ)=P11−y1=0.5−0=0.5 
h1=l''(y1,y1ˆ)=P11(1−P11)=0.5（1−0.5）=0.25 

接下来我们需要在特征x1、x2中寻找最佳划分点.

以x1为例：我们需要将x1的特征值从小到大排列，一共有{1,2,3,6,7} 中取值。 

**当以特征值为1作为划分点时（x1<1）**:

左子树集合为 I_left = {}

右子树集合为 I_right = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}
$$
G_L=\sum_{i∈I_{left}}gi=0(因为左子叶为空集)\\
H_L=\sum_{i∈I_{left}}h_i=0\\
G_R=\sum_{i∈I_{right}}gi=-1\\
H_R=\sum_{i∈I_{right}}h_i=2.5\\
$$
最后计算增益：
$$
Gini=\frac{1}{2}[\frac{G_L^2}{H_L+λ}+\frac{G_R^2}{H_R+λ}-\frac{(G_L+G_R)^2}{H_L+H_R+λ}]-γ\\
=\frac{1}{2}[\frac{0}{0+λ}+\frac{1}{2.5+λ}-\frac{1}{2.5+λ}]-γ=0\\ 
=\frac{1}{2}[\frac{0}{0+1}+\frac{1}{2.5+1}-\frac{1}{2.5+1}]-0=0\\
-\\
γ=0;λ=1
$$
这是显然的，因为左子树是空集，相当于没有对数据集进行划分

**当以特征值为2作为划分点时（x1<2）**:

左子树集合为 I_left = {1, 4}

右子树集合为 I_right = {2, 3, 5, 6, 7, 8, 9, 10}


$$
G_L=\sum_{i∈I_{left}}gi=0\\
H_L=\sum_{i∈I_{left}}h_i=0.5\\
G_R=\sum_{i∈I_{right}}gi=-1\\
H_R=\sum_{i∈I_{right}}h_i=2\\
$$
最后计算增益：
$$
Gini=\frac{1}{2}[\frac{G_L^2}{H_L+λ}+\frac{G_R^2}{H_R+λ}-\frac{(G_L+G_R)^2}{H_L+H_R+λ}]-γ\\
=\frac{1}{2}[\frac{0}{0.5+λ}+\frac{1}{2+λ}-\frac{1}{0.5+2+λ}]-γ=0\\ 
=\frac{1}{2}[\frac{0}{0.5+1}+\frac{1}{2+1}-\frac{1}{0.5+2+1}]-0=0.023809\\
-\\
γ=0;λ=1
$$

$$
\frac{1}{3}-\frac{2}{7}=\frac{7-6}{21}=\frac{1}{21}\\
\frac{1}{2}*\frac{1}{21}=\frac{1}{42}=0.023809
$$

依次算出x1的各个特征值的参数，如下表：

| Split_point |  1   |    2     |    3     |     6     |  7   |
| :---------: | :--: | :------: | :------: | :-------: | :--: |
|     G_L     |  0   |    0     |    0     |   -0.5    | -1.0 |
|     H_L     |  0   |   0.5    |   1.0    |   1.25    | 2.5  |
|     G_R     | -1.0 |   -1.0   |   -1.0   |   -0.5    | 0.0  |
|     H_R     | 2.5  |    2     |   1.5    |   1.25    | 0.0  |
|    Gain     | 0.0  | 0.023809 | 0.057142 | -0.031746 | 0.0  |

因此x1的特征下的最佳划分点为x1<3，此时得到的增益最大。

**若以x2为例：**

x2的可能取值排序为{−5，−2，0，2，5}
当以特征值为-5作为划分点时（x2<-5): 

**当以特征值为2作为划分点时（x1<2）**:

左子树集合为 I_left = {}

右子树集合为 I_right = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}


$$
G_L=\sum_{i∈I_{left}}gi=0\\
H_L=\sum_{i∈I_{left}}h_i=0\\
G_R=\sum_{i∈I_{right}}gi=-1.0\\
H_R=\sum_{i∈I_{right}}h_i=2.5\\
$$
最后计算增益：
$$
Gini=\frac{1}{2}[\frac{G_L^2}{H_L+λ}+\frac{G_R^2}{H_R+λ}-\frac{(G_L+G_R)^2}{H_L+H_R+λ}]-γ\\
=\frac{1}{2}[\frac{0}{0+λ}+\frac{1}{2.5+λ}-\frac{1}{2.5+λ}]-γ=0\\ 
=\frac{1}{2}[\frac{0}{0+1}+\frac{1}{2.5+1}-\frac{1}{2.5+1}]-0=0\\
-\\
γ=0;λ=1
$$
当以特征值为-2作为划分点时（x2<-2):

左子树集合为 I_left = {1, 6}

右子树集合为 I_right = {2, 3, 4, 5,  7, 8, 9, 10}


$$
G_L=\sum_{i∈I_{left}}gi=0\\
H_L=\sum_{i∈I_{left}}h_i=0.5\\
G_R=\sum_{i∈I_{right}}gi=-1.0\\
H_R=\sum_{i∈I_{right}}h_i=2.0\\
$$
最后计算增益：
$$
Gini=\frac{1}{2}[\frac{G_L^2}{H_L+λ}+\frac{G_R^2}{H_R+λ}-\frac{(G_L+G_R)^2}{H_L+H_R+λ}]-γ=0.023809
$$
然后也是依次计算出特征x2的各个划分点的参数与增益值，列表如下：

| Split_point |  -5  |    -2    |    0     |     2     |  5   |
| :---------: | :--: | :------: | :------: | :-------: | :--: |
|     G_L     |  0   |    0     |    0     |   -0.5    | -1.0 |
|     H_L     |  0   |   0.5    |   1.0    |   1.25    | 2.5  |
|     G_R     | -1.0 |   -1.0   |   -1.0   |   -0.5    | 0.0  |
|     H_R     | 2.5  |    2     |   1.5    |   1.25    | 0.0  |
|    Gain     | 0.0  | 0.023809 | 0.057142 | -0.031746 | 0.0  |

因此对特征x2而言，最佳划分点位x2<0

因为x1与x2的最佳划分点的增益值相同，所以我们选取x1<3作为最佳划分点即可。

那么划分后左子树的样本集合为：I_left={1,2,4,5},I_right={3,6,7,8,9,10}
故左叶子结点的权值为： 
$$
w_{1左}^*=-\frac{G}{H+λ}η=-\frac{0}{1+1}*0.1=0\\
w_{1右}^*=-\frac{G}{H+λ}η=-\frac{-1.0}{1.5+1}*0.1=0.04\
$$
 *η*是学习率，防止过拟合。

至此第一颗树建立完毕,如果深度为2，那么就需在左节点与右节点分别重复上面的步骤即可。

2、生成第二棵树

回想一下，我们在生成第一颗树的时候用到了f0(xi) 

这颗初始的树是我们自己设置的（base_score=P11 =0.5） 

假设模型只有这一颗树（T=1），那么模型对样本xi 进行预测的值是什么呢？

由加法模型知： 
$$
y_i^T=\sum_{t=0}^{T}f_t(x_i)\\
y_i^1=f_0(x_i)+f_1(x_i)
$$


f_1(xi)是我们样本xi 落在第一颗树上的某个节点的值，而f0(xi) 就是前面提到的base_score经过sigmod映射后的值，因为我们选择的是logloss做损失函数，所以概率为
$$
P=\frac{1}{1+e^{-x}}\\
我们用0.5做逆运算 x=ln\frac{y}{1-y}后可以得到f_0(x_i)=0\\
$$
因此第一棵树的预测结果为：
$$
y_i^1=f_0(x_i)+f_1(x_i)=0+w_{q(x_i)}
$$
官方文档有说明，当生成的树较多时（T很大），那么初始值几乎不起什么作用。


$$
\widehat{y}_i^2=\widehat{y}_i^1+f_2(x_i)
$$
因此我们需要对第一颗树的结果做映射，映射后的值就是我们计算logloss的一阶、二阶导数中的P12
第二颗logloss的一阶、二阶导数为：
$$
g_i=l'(y_i,\widehat{y}_i)=P_{12}-y_i\\
h_i=l''(y_i,\widehat{y}_i)=P_{12}(1-P_{12})
$$
则P12的值如下表： 

 

|  ID  |  P12   |
| :--: | :----: |
|  1   |  0.5   |
|  2   |  0.5   |
|  3   | 0.5099 |
|  4   |  0.5   |
|  5   |  0.5   |
|  6   | 0.5099 |
|  7   | 0.5099 |
|  8   | 0.5099 |
|  9   | 0.5099 |
|  10  | 0.5099 |

$$
说明\\
P=\frac{1}{1+e^{-0}}=0.5\\
P=\frac{1}{1+e^{-0.04}}=0.5099\\
$$

后面的树的生成与第一颗树一模一样，记得最后需要对预测值做映射。

