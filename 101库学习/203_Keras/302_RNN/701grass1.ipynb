{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recurrentshop.cells import *\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, Activation\n",
    "from keras.layers import add, multiply, concatenate\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoderCell(ExtendedRNNCell):\n",
    "\n",
    "    def __init__(self, hidden_dim=None, **kwargs):\n",
    "        if hidden_dim:\n",
    "            self.hidden_dim = hidden_dim\n",
    "        else:\n",
    "            self.hidden_dim = self.output_dim\n",
    "        super(LSTMDecoderCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build_model(self, input_shape):\n",
    "        hidden_dim = self.hidden_dim\n",
    "        output_dim = self.output_dim\n",
    "\n",
    "        x = Input(batch_shape=input_shape)\n",
    "        h_tm1 = Input(batch_shape=(input_shape[0], hidden_dim))\n",
    "        c_tm1 = Input(batch_shape=(input_shape[0], hidden_dim))\n",
    "\n",
    "        W1 = Dense(hidden_dim * 4,\n",
    "                   kernel_initializer=self.kernel_initializer,\n",
    "                   kernel_regularizer=self.kernel_regularizer,\n",
    "                   use_bias=False)\n",
    "        W2 = Dense(output_dim,\n",
    "                   kernel_initializer=self.kernel_initializer,\n",
    "                   kernel_regularizer=self.kernel_regularizer,)\n",
    "        U = Dense(hidden_dim * 4,\n",
    "                  kernel_initializer=self.kernel_initializer,\n",
    "                  kernel_regularizer=self.kernel_regularizer,)\n",
    "\n",
    "        z = add([W1(x), U(h_tm1)])\n",
    "\n",
    "        z0, z1, z2, z3 = get_slices(z, 4)\n",
    "        i = Activation(self.recurrent_activation)(z0)\n",
    "        f = Activation(self.recurrent_activation)(z1)\n",
    "        c = add([multiply([f, c_tm1]), multiply([i, Activation(self.activation)(z2)])])\n",
    "        o = Activation(self.recurrent_activation)(z3)\n",
    "        h = multiply([o, Activation(self.activation)(c)])\n",
    "        y = Activation(self.activation)(W2(h))\n",
    "\n",
    "        return Model([x, h_tm1, c_tm1], [y, h, c])\n",
    "\n",
    "\n",
    "class AttentionDecoderCell(ExtendedRNNCell):\n",
    "\n",
    "    def __init__(self, hidden_dim=None, **kwargs):\n",
    "        if hidden_dim:\n",
    "            self.hidden_dim = hidden_dim\n",
    "        else:\n",
    "            self.hidden_dim = self.output_dim\n",
    "        self.input_ndim = 3\n",
    "        super(AttentionDecoderCell, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def build_model(self, input_shape):\n",
    "        \n",
    "        input_dim = input_shape[-1]\n",
    "        output_dim = self.output_dim\n",
    "        input_length = input_shape[1]\n",
    "        hidden_dim = self.hidden_dim\n",
    "\n",
    "        x = Input(batch_shape=input_shape)\n",
    "        h_tm1 = Input(batch_shape=(input_shape[0], hidden_dim))\n",
    "        c_tm1 = Input(batch_shape=(input_shape[0], hidden_dim))\n",
    "        \n",
    "        W1 = Dense(hidden_dim * 4,\n",
    "                   kernel_initializer=self.kernel_initializer,\n",
    "                   kernel_regularizer=self.kernel_regularizer)\n",
    "        W2 = Dense(output_dim,\n",
    "                   kernel_initializer=self.kernel_initializer,\n",
    "                   kernel_regularizer=self.kernel_regularizer)\n",
    "        W3 = Dense(1,\n",
    "                   kernel_initializer=self.kernel_initializer,\n",
    "                   kernel_regularizer=self.kernel_regularizer)\n",
    "        U = Dense(hidden_dim * 4,\n",
    "                  kernel_initializer=self.kernel_initializer,\n",
    "                  kernel_regularizer=self.kernel_regularizer)\n",
    "\n",
    "        C = Lambda(lambda x: K.repeat(x, input_length), output_shape=(input_length, input_dim))(c_tm1)\n",
    "        _xC = concatenate([x, C])\n",
    "        _xC = Lambda(lambda x: K.reshape(x, (-1, input_dim + hidden_dim)), output_shape=(input_dim + hidden_dim,))(_xC)\n",
    "\n",
    "        alpha = W3(_xC)\n",
    "        alpha = Lambda(lambda x: K.reshape(x, (-1, input_length)), output_shape=(input_length,))(alpha)\n",
    "        alpha = Activation('softmax')(alpha)\n",
    "\n",
    "        _x = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=(1, 1)), output_shape=(input_dim,))([alpha, x])\n",
    "\n",
    "        z = add([W1(_x), U(h_tm1)])\n",
    "\n",
    "        z0, z1, z2, z3 = get_slices(z, 4)\n",
    "\n",
    "        i = Activation(self.recurrent_activation)(z0)\n",
    "        f = Activation(self.recurrent_activation)(z1)\n",
    "\n",
    "        c = add([multiply([f, c_tm1]), multiply([i, Activation(self.activation)(z2)])])\n",
    "        o = Activation(self.recurrent_activation)(z3)\n",
    "        h = multiply([o, Activation(self.activation)(c)])\n",
    "        y = Activation(self.activation)(W2(h))\n",
    "\n",
    "        return Model([x, h_tm1, c_tm1], [y, h, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from recurrentshop import LSTMCell, RecurrentSequential \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, TimeDistributed, Bidirectional, Input, Flatten\n",
    "import os\n",
    "import codecs\n",
    "# 载入模型\n",
    "from keras_bert import load_trained_model_from_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义地址\n",
    "bert_model_path = \"/Users/zhouwencheng/Desktop/Grass/data/model/ImportModel/BERT/chinese_L-12_H-768_A-12\"\n",
    "config_path = os.path.join(bert_model_path, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(bert_model_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(bert_model_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0812 01:30:02.673328 4321588096 deprecation_wrapper.py:119] From /Users/zhouwencheng/Desktop/Grass/02Study/02PythonEnv/envpy3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0812 01:30:02.707922 4321588096 deprecation_wrapper.py:119] From /Users/zhouwencheng/Desktop/Grass/02Study/02PythonEnv/envpy3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0812 01:30:02.811918 4321588096 deprecation_wrapper.py:119] From /Users/zhouwencheng/Desktop/Grass/02Study/02PythonEnv/envpy3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0812 01:30:02.813101 4321588096 deprecation_wrapper.py:119] From /Users/zhouwencheng/Desktop/Grass/02Study/02PythonEnv/envpy3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0812 01:30:02.832684 4321588096 deprecation.py:506] From /Users/zhouwencheng/Desktop/Grass/02Study/02PythonEnv/envpy3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0812 01:30:02.890269 4321588096 deprecation_wrapper.py:119] From /Users/zhouwencheng/Desktop/Grass/02Study/02PythonEnv/envpy3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_bert = load_trained_model_from_checkpoint(config_path, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_bert.add(Dense(1000))\n",
    "input_1 = Input(batch_shape=(None, 512))\n",
    "model_output = model_bert([input_1, input_1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot add an empty model to a `Sequential` model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-7002f984ae43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# model_decoder = Model(inputs, decoded)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Grass/02Study/02PythonEnv/envpy3.7/lib/python3.7/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     \u001b[0;31m# input shape and dtype.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                         raise ValueError('Cannot add an empty model '\n\u001b[0m\u001b[1;32m    147\u001b[0m                                          'to a `Sequential` model.')\n\u001b[1;32m    148\u001b[0m                     \u001b[0;31m# In case of nested models: recover the first layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot add an empty model to a `Sequential` model."
     ]
    }
   ],
   "source": [
    "input_2= Input(batch_shape=(None, 512))\n",
    "model_2=Model(model_output)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(model_2)\n",
    "\n",
    "# model_decoder = Model(inputs, decoded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x15d7b9b00>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The added layer must be an instance of class Layer. Found: Tensor(\"model_2_7/Encoder-12-FeedForward-Norm/add_1:0\", shape=(?, 512, 768), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-848484c8b943>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0minput_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Grass/02Study/02PythonEnv/envpy3.7/lib/python3.7/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    130\u001b[0m             raise TypeError('The added layer must be '\n\u001b[1;32m    131\u001b[0m                             \u001b[0;34m'an instance of class Layer. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                             'Found: ' + str(layer))\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Found: Tensor(\"model_2_7/Encoder-12-FeedForward-Norm/add_1:0\", shape=(?, 512, 768), dtype=float32)"
     ]
    }
   ],
   "source": [
    "input_1 = Input(batch_shape=(None, 512))\n",
    "model = Sequential()\n",
    "model.add(model_output)\n",
    "input_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x13902bac8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model(model_decoder.inputs, model_decoder.outputs)\n",
    "# model_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_12 (Model)             (None, 128, 21100)        2942489   \n",
      "=================================================================\n",
      "Total params: 2,942,489\n",
      "Trainable params: 2,942,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_1 = Input(batch_shape=(512,))\n",
    "input_2 = Input(batch_shape=(512,None, None))\n",
    "\n",
    "model_bert\n",
    "model = Sequential()\n",
    "model.add(Model(model_decoder.inputs, model_decoder.outputs))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_67:0' shape=(?, 93, 300) dtype=float32>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded.shape: (None, 512, 768)\n",
      "(?, 512, 768)\n",
      "(?, 512, 768)\n"
     ]
    }
   ],
   "source": [
    "output_dim=21100\n",
    "output_length=128\n",
    "batch_input_shape=None\n",
    "batch_size=None\n",
    "input_shape=None\n",
    "input_length=512\n",
    "input_dim=768\n",
    "hidden_dim=128 \n",
    "depth=1\n",
    "bidirectional=True\n",
    "unroll=False\n",
    "stateful=False\n",
    "dropout=0.0\n",
    "shape=(None, 512, 768)\n",
    "\n",
    "_input = Input(batch_shape=shape)\n",
    "_input._keras_history[0].supports_masking = True\n",
    " \n",
    "\n",
    "print(\"encoded.shape:\", shape)\n",
    "decoder = RecurrentSequential(decode=True, output_length=output_length,\n",
    "                              unroll=unroll, stateful=stateful)\n",
    "decoder.add(Dropout(dropout, batch_input_shape=(shape[0], shape[1], hidden_dim)))\n",
    "decoder.add(AttentionDecoderCell(output_dim=output_dim, hidden_dim=hidden_dim))\n",
    "\n",
    "\n",
    "\n",
    "print(model_bert.outputs[0].shape)\n",
    "print(_input.shape)\n",
    "\n",
    "inputs = [_input]\n",
    "# _input = model_bert.outputs[0]\n",
    "\n",
    "decoded = decoder(_input) \n",
    "# model_decoder = Model(inputs, decoded) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(model_decoder)\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1000)) \n",
    "\n",
    "    \n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'recurrent_sequential_7/transpose_1:0' shape=(?, ?, 21100) dtype=float32>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim=300\n",
    "output_length=5000\n",
    "batch_input_shape=None\n",
    "batch_size=None\n",
    "input_shape=None\n",
    "input_length=93\n",
    "input_dim=300\n",
    "hidden_dim=128 \n",
    "depth=1\n",
    "bidirectional=True\n",
    "unroll=False\n",
    "stateful=False\n",
    "dropout=0.0\n",
    "\n",
    "shape=(93, None, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded.shape: (None, 93, 300)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_726 (InputLayer)          (None, 93, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "recurrent_sequential_56 (Recurr (None, 5000, 300)    259289      input_726[0][0]                  \n",
      "                                                                 private__optional_input_place_hol\n",
      "                                                                 private__optional_input_place_hol\n",
      "==================================================================================================\n",
      "Total params: 259,289\n",
      "Trainable params: 259,289\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if isinstance(depth, int):\n",
    "    depth = (depth, depth)\n",
    "if batch_input_shape:\n",
    "    shape = batch_input_shape\n",
    "elif input_shape:\n",
    "    shape = (batch_size,) + input_shape\n",
    "elif input_dim:\n",
    "    if input_length:\n",
    "        shape = (batch_size,) + (input_length,) + (input_dim,)\n",
    "    else:\n",
    "        shape = (batch_size,) + (None,) + (input_dim,)\n",
    "else:\n",
    "    # TODO Proper error message\n",
    "    raise TypeError\n",
    "if hidden_dim is None:\n",
    "    hidden_dim = output_dim\n",
    "\n",
    "_input = Input(batch_shape=shape)\n",
    "_input._keras_history[0].supports_masking = True\n",
    "\n",
    "encoder = RecurrentSequential(unroll=unroll, stateful=stateful,\n",
    "                              return_sequences=True)\n",
    "encoder.add(LSTMCell(hidden_dim, batch_input_shape=(shape[0], shape[2])))\n",
    "\n",
    "for _ in range(1, depth[0]):\n",
    "    encoder.add(Dropout(dropout))\n",
    "    encoder.add(LSTMCell(hidden_dim))\n",
    "\n",
    "if bidirectional:\n",
    "    encoder = Bidirectional(encoder, merge_mode='sum')\n",
    "    encoder.forward_layer.build(shape)\n",
    "    encoder.backward_layer.build(shape)\n",
    "    # patch\n",
    "    encoder.layer = encoder.forward_layer\n",
    "\n",
    "encoded = encoder(_input)\n",
    "\n",
    "print(\"encoded.shape:\", shape)\n",
    "decoder = RecurrentSequential(decode=True, output_length=output_length,\n",
    "                              unroll=unroll, stateful=stateful)\n",
    "decoder.add(Dropout(dropout, batch_input_shape=(shape[0], shape[1], hidden_dim)))\n",
    "if depth[1] == 1:\n",
    "    decoder.add(AttentionDecoderCell(output_dim=output_dim, hidden_dim=hidden_dim))\n",
    "else:\n",
    "    decoder.add(AttentionDecoderCell(output_dim=output_dim, hidden_dim=hidden_dim))\n",
    "    for _ in range(depth[1] - 2):\n",
    "        decoder.add(Dropout(dropout))\n",
    "        decoder.add(LSTMDecoderCell(output_dim=hidden_dim, hidden_dim=hidden_dim))\n",
    "    decoder.add(Dropout(dropout))\n",
    "    decoder.add(LSTMDecoderCell(output_dim=output_dim, hidden_dim=hidden_dim))\n",
    "\n",
    "inputs = [_input]\n",
    "decoded = decoder(_input)\n",
    "# decoded = decoder(encoded)\n",
    "model = Model(inputs, decoded)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
