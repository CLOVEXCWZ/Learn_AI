{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('学', 1), ('习', 1), ('k', 2), ('e', 3), ('r', 2), ('a', 1), ('s', 1), ('的', 1), ('t', 1), ('o', 1), ('n', 1), ('i', 1), ('z', 1), ('就', 1), ('是', 1), ('这', 1), ('么', 1), ('简', 1), ('单', 1)])\n",
      "defaultdict(<class 'int'>, {'t': 1, 'o': 1, 'r': 1, 'n': 1, '习': 1, '学': 1, 's': 1, 'k': 1, 'a': 1, 'z': 1, 'e': 1, '的': 1, 'i': 1, '么': 1, '这': 1, '简': 1, '就': 1, '单': 1, '是': 1})\n",
      "2\n",
      "{'UNK': 1, 'e': 2, 'k': 3, 'r': 4, '学': 5, '习': 6, 'a': 7, 's': 8, '的': 9, 't': 10, 'o': 11, 'n': 12, 'i': 13, 'z': 14, '就': 15, '是': 16, '这': 17, '么': 18, '简': 19, '单': 20}\n",
      "[[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[5, 6, 3, 2, 4, 7, 8, 9, 10, 11, 3, 2, 12, 13, 14, 2, 4], [15, 16, 17, 18, 19, 20]]\n",
      "[[ 5  6  3  2  4  7  8  9 10 11  3  2 12 13 14  2  4  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]\n",
      " [15 16 17 18 19 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "text1 = \"学习keras的Tokenizer\"\n",
    "text2 = \"就是这么简单\"\n",
    "texts = [text1, text2]\n",
    "\n",
    "\"\"\"\n",
    "# num_words 表示用多少词语生成词典（vocabulary）\n",
    "# char_level表示 if True, every character will be treated as a token.\n",
    "# oov_token是out-of-vocabulary，用来代替那些字典上没有的字。\n",
    "\"\"\"\n",
    "tokenizer = Tokenizer(num_words=5000, char_level=True, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# 可以设置词典\n",
    "# tokenizer.word_index = {'UNK': 1, '学': 2, '习': 3}\n",
    "\n",
    "# 每个word出现了几次\n",
    "print(tokenizer.word_counts)\n",
    "# 每个word出现在几个文档中\n",
    "print(tokenizer.word_docs)\n",
    "# 每个word出现了几次\n",
    "print(tokenizer.document_count)\n",
    "# 每个word对应的index，字典映射\n",
    "print(tokenizer.word_index)\n",
    "# mode：‘binary’，‘count’，‘tfidf’，‘freq’之一，默认为‘binary’\n",
    "# 返回值：形如(len(texts), nb_words)的numpy array\n",
    "print(tokenizer.texts_to_matrix(texts))\n",
    "# 序列的列表\n",
    "print(tokenizer.texts_to_sequences(texts))\n",
    "texts = tokenizer.texts_to_sequences(texts)\n",
    "texts = sequence.pad_sequences(texts, maxlen=30, padding='post',truncating='post')\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今天', '北京', '下', '暴雨', '了']\n",
      "2\n",
      "OrderedDict([('今天', 2), ('北京', 1), ('下', 1), ('暴雨', 1), ('了', 1), ('我', 1), ('打车', 1), ('回家', 1)])\n",
      "defaultdict(<class 'int'>, {'了': 1, '下': 1, '暴雨': 1, '今天': 2, '北京': 1, '打车': 1, '回家': 1, '我': 1})\n",
      "{'今天': 1, '北京': 2, '下': 3, '暴雨': 4, '了': 5, '我': 6, '打车': 7, '回家': 8}\n",
      "defaultdict(<class 'int'>, {5: 1, 3: 1, 4: 1, 1: 2, 2: 1, 7: 1, 8: 1, 6: 1})\n",
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n",
      "[[1, 2, 3, 4, 5], [6, 1, 7, 8]]\n",
      "[[0 0 0 0 0 0 0 1 2 3]\n",
      " [0 0 0 0 0 0 0 4 5 6]]\n",
      "[[1 2 3 0 0 0 0 0 0 0]\n",
      " [4 5 6 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "text1 = \"今天 北京 下 暴雨 了\"\n",
    "text2 = \"我 今天 打车 回家\"\n",
    "texts = [text1, text2]\n",
    "\n",
    "print(text_to_word_sequence(text1))  # 按空格分割语料\n",
    "# ['今天', '北京', '下', '暴雨', '了']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "print(tokenizer.document_count) # 处理文档的数量\n",
    "# 2\n",
    "print(tokenizer.word_counts) # 词频字典，按词频从大到小排序\n",
    "# OrderedDict([('今天', 2), ('北京', 1), ('下', 1), ('暴雨', 1), ('了', 1), ('我', 1), ('打车', 1), ('回家', 1)])\n",
    "print(tokenizer.word_docs) # 保存每个word出现的文档的数量\n",
    "# {'了': 1, '暴雨': 1, '北京': 1, '下': 1, '今天': 2, '打车': 1, '回家': 1, '我': 1}\n",
    "print(tokenizer.word_index) # 给每个词唯一id\n",
    "# {'今天': 1, '北京': 2, '下': 3, '暴雨': 4, '了': 5, '我': 6, '打车': 7, '回家': 8}\n",
    "print(tokenizer.index_docs) # 保存word的id出现的文档的数量\n",
    "# {5: 1, 4: 1, 2: 1, 3: 1, 1: 2, 7: 1, 8: 1, 6: 1}\n",
    "print(tokenizer.texts_to_matrix(texts))\n",
    "# [[0. 1. 1. ... 0. 0. 0.]\n",
    "# [0. 1. 0. ... 0. 0. 0.]]\n",
    "# shape = (2, 5000)\n",
    "print(tokenizer.texts_to_sequences(texts))\n",
    "# [[1, 2, 3, 4, 5],\n",
    "#  [6, 1, 7, 8] ] \n",
    "\n",
    "\n",
    "# 将序列填充到maxlen长度\n",
    "print(pad_sequences([[1,2,3],[4,5,6]],maxlen=10,padding='pre')) # 在序列前填充\n",
    "# [[0 0 0 0 0 0 0 1 2 3]\n",
    "# [0 0 0 0 0 0 0 4 5 6]]\n",
    "print(pad_sequences([[1,2,3],[4,5,6]],maxlen=10,padding='post')) # 在序列后填充\n",
    "# [[1 2 3 0 0 0 0 0 0 0]\n",
    "# [4 5 6 0 0 0 0 0 0 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
