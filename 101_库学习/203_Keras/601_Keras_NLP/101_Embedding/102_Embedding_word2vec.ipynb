{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Word2Vec\n",
    "\n",
    "**[摘要] word2vec是NLP文字转向量很重要的组成，此次采用 gensim 加载训练好的vec变成参数加载到Embedding的初始化参数中（vec向量是由维基百科训练得到的向量，具体没有过多介绍---）。这边便能达到embeddin操作的目的，同时加载到Embedding的参数还可以选择再训练**\n",
    "\n",
    "[参考源码地址==========](https://github.com/yongzhuo/Keras-TextClassification)\n",
    "\n",
    "\n",
    "word2vec是在NLP向量化非常普遍的形式，由于计算资源等因素能采用别人训练好的向量既可以达到向量化的目的，又可以不用再花太多时间这可以说是一举两得的！如果不是领域行非常强的话可以做一些微调即可很好的使用，如果领域性很强则需要考虑是否需要自己重先训练。\n",
    "\n",
    "这里记录下今天（2019-11-27）探索Word2Vec的实现代码，以便于以后有需要的时候能快速实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/zhouwencheng/Desktop/Grass/data/model/ImportModel/Word2Vec/w2v_model_wiki_char.vec'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Add, Embedding\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.models import Input, Model\n",
    "\n",
    "import numpy as np\n",
    "import codecs\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 通过维基百科训练出来的词向量，维度为300W（字符级）\n",
    "path_embedding_w2v_wiki = \"/Users/zhouwencheng/Desktop/Grass/data/model\" \\\n",
    "                          \"/ImportModel/Word2Vec/w2v_model_wiki_char.vec\"\n",
    "path_embedding_w2v_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(object):\n",
    "    def __init__(self,\n",
    "                 len_max=50,  # 文本最大长度, 建议25-50\n",
    "                 embed_size=300,  # 嵌入层尺寸\n",
    "                 vocab_size=30000,  # 字典大小, 这里随便填的，会根据代码里修改\n",
    "                 trainable=True,  # 是否训练参数\n",
    "                 path_vec=path_embedding_w2v_wiki,\n",
    "                ):\n",
    "        self.len_max = len_max\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.trainable = trainable\n",
    "        self.path_vec = path_vec\n",
    "        \n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.model = None\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = {}\n",
    "        \n",
    "        # 定义符号\n",
    "        self.ot_dict = {\n",
    "            '[PAD]': 0,\n",
    "            '[UNK]': 1,\n",
    "            '[BOS]': 2,\n",
    "            '[EOS]': 3, }\n",
    "        self.deal_corpus()\n",
    "        self.build()\n",
    "        \n",
    "    def deal_corpus(self): \n",
    "        pass\n",
    "    \n",
    "    def build(self, **kwargs):\n",
    "        print(f\"load word2vec start!\")\n",
    "        self.key_vector = KeyedVectors.load_word2vec_format(self.path_vec, **kwargs)\n",
    "        print(f\"load word2vec end!\")\n",
    "        self.embed_size = self.key_vector.vector_size\n",
    "        self.token2idx = self.ot_dict.copy()\n",
    "        embedding_matrix = []\n",
    "        # 首先加self.token2idx中的四个[PAD]、[UNK]、[BOS]、[EOS]\n",
    "        embedding_matrix.append(np.zeros(self.embed_size))\n",
    "        embedding_matrix.append(np.random.uniform(-0.5, 0.5, self.embed_size))\n",
    "        embedding_matrix.append(np.random.uniform(-0.5, 0.5, self.embed_size))\n",
    "        embedding_matrix.append(np.random.uniform(-0.5, 0.5, self.embed_size))\n",
    "        for word in self.key_vector.index2entity:\n",
    "            self.token2idx[word] = len(self.token2idx)\n",
    "            embedding_matrix.append(self.key_vector[word])\n",
    "\n",
    "        self.idx2token = {}\n",
    "        for key, value in self.token2idx.items():\n",
    "            self.idx2token[value] = key\n",
    "\n",
    "        self.vocab_size = len(self.token2idx)\n",
    "        embedding_matrix = np.array(embedding_matrix)\n",
    "        self.input = Input(shape=(self.len_max, ), dtype='int32')\n",
    "        self.output = Embedding(\n",
    "            input_dim=self.vocab_size,\n",
    "            output_dim=self.embed_size,\n",
    "            input_length=self.len_max,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=self.trainable\n",
    "        )(self.input)\n",
    "        self.model = Model(inputs=self.input, outputs=self.output)\n",
    "    \n",
    "    def sentence2idx(self, text):\n",
    "        text = self.extract_chinese(str(text)).upper()\n",
    "        text = list(text)\n",
    "        text = [text_one for text_one in text]\n",
    "        len_leave = self.len_max - len(text)\n",
    "\n",
    "        # 转换和填充处理\n",
    "        if len_leave >= 0:\n",
    "            text_index = [self.token2idx[text_char] if text_char in self.token2idx else self.token2idx['[UNK]'] for text_char in text] + [self.token2idx['[PAD]'] for i in range(len_leave)]\n",
    "        else:\n",
    "            text_index = [self.token2idx[text_char] if text_char in self.token2idx else self.token2idx['[UNK]'] for\n",
    "                          text_char in text[0:self.len_max]]\n",
    "        return text_index\n",
    "    \n",
    "    def idx2sentence(self, idx):\n",
    "        assert type(idx) == list\n",
    "        text_idx = [self.idx2token[id] if id in self.idx2token else self.idx2token['[UNK]'] for id in idx]\n",
    "        return \"\".join(text_idx)\n",
    "    \n",
    "    def extract_chinese(self, text):\n",
    "        \"\"\"\n",
    "              只提取出中文、字母和数字\n",
    "            :param text: str, input of sentence\n",
    "            :return:\n",
    "            \"\"\"\n",
    "        chinese_exttract = ''.join(re.findall(u\"([\\u4e00-\\u9fa5A-Za-z0-9@._])\", text))\n",
    "        return chinese_exttract   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load word2vec start!\n",
      "load word2vec end!\n",
      "WARNING:tensorflow:From /Users/zhouwencheng/Desktop/Grass/101PythonEnv/envpy3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zhouwencheng/Desktop/Grass/101PythonEnv/envpy3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zhouwencheng/Desktop/Grass/101PythonEnv/envpy3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zhouwencheng/Desktop/Grass/101PythonEnv/envpy3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zhouwencheng/Desktop/Grass/101PythonEnv/envpy3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "(2, 50)\n",
      "(2, 50, 300)\n",
      "[[[-1.2650331   3.1101494  -2.2554128  ...  1.818751    3.429298\n",
      "   -2.7108421 ]\n",
      "  [ 0.21647607 -3.4711666  -1.4919875  ...  4.7559776   0.2984004\n",
      "    0.40304002]\n",
      "  [ 0.21647607 -3.4711666  -1.4919875  ...  4.7559776   0.2984004\n",
      "    0.40304002]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.20198439  0.06784999 -1.4983975  ...  0.9137133   2.7521787\n",
      "   -0.21572655]\n",
      "  [ 0.21647607 -3.4711666  -1.4919875  ...  4.7559776   0.2984004\n",
      "    0.40304002]\n",
      "  [ 0.21647607 -3.4711666  -1.4919875  ...  4.7559776   0.2984004\n",
      "    0.40304002]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]]\n",
      "(2, 50, 300)\n"
     ]
    }
   ],
   "source": [
    "texts = [\"今天天气不错\",\n",
    "             \"明天天气也不错\"]\n",
    "eb = WordEmbedding()\n",
    "x = []\n",
    "for t in texts:\n",
    "    x.append(eb.sentence2idx(t))\n",
    "x = np.array(x)\n",
    "print(x.shape)\n",
    "\n",
    "model = eb.model\n",
    "p = model.predict(x)\n",
    "print(p.shape)\n",
    "print(p)\n",
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
