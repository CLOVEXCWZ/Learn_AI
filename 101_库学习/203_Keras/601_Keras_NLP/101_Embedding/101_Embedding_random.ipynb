{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Random\n",
    "\n",
    "**摘要:**随机Embedding其实就是在Embedding层的参数进行随机初始化的过程,本次主要对随机Embedding的整个过程进行记录，以便以后有需要的时候可以很快的复现。\n",
    "\n",
    "[参考源码地址--------](https://github.com/yongzhuo/Keras-TextClassification)\n",
    "\n",
    "采用随机生成参数的形式进行Embedding操作，可在训练中更新参数以达到训练的目的，在word2vec对Embedd的初始化过程都是采用随机生成的，然后在训练中去不断更新参数，以达到训练的目的。\n",
    "\n",
    "随机Embedding其实就是在Embedding层的参数进行随机初始化的过程\n",
    "\n",
    "**注意:本次实例是字符级别的Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding \n",
    "from keras.models import Input, Model\n",
    "\n",
    "import numpy as np  \n",
    "import re\n",
    "\n",
    "# 字符字典文件\n",
    "path_embedding_term_char = \"/Users/zhouwencheng/Desktop/Grass/data/model\" \\\n",
    "                           \"/ImportModel/Word2Vec/term_char.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomEmbedding(object):\n",
    "    def __init__(self,\n",
    "                 len_max=50,  # 文本最大长度, 建议25-50\n",
    "                 embed_size=300,  # 嵌入层尺寸\n",
    "                 vocab_size=30000,  # 字典大小, 这里随便填的，会根据代码里修改\n",
    "                 trainable=True,  # 是否训练参数\n",
    "                 path_char=path_char,\n",
    "                ):\n",
    "        self.len_max = len_max\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.trainable = trainable\n",
    "        self.path_char = path_char\n",
    "        \n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.model = None\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = {}\n",
    "        \n",
    "        # 定义符号\n",
    "        self.ot_dict = {\n",
    "            '[PAD]': 0,\n",
    "            '[UNK]': 1,\n",
    "            '[BOS]': 2,\n",
    "            '[EOS]': 3, }\n",
    "        self.deal_corpus()\n",
    "        self.build()\n",
    "        \n",
    "    def deal_corpus(self):\n",
    "        token2idx = self.ot_dict.copy()\n",
    "        count = 3\n",
    "        with open(file=self.path_char, mode='r', encoding='utf-8') as fd:\n",
    "            while True:\n",
    "                term_one = fd.readline()\n",
    "                if not term_one:\n",
    "                    break\n",
    "                term_one = term_one.strip()\n",
    "                if term_one not in token2idx:\n",
    "                    count = count + 1\n",
    "                    token2idx[term_one] = count\n",
    "        self.token2idx = token2idx\n",
    "        self.idx2token = {}\n",
    "        for key, value in self.token2idx.items():\n",
    "            self.idx2token[value] = key\n",
    "    \n",
    "    def build(self, **kwargs):\n",
    "        self.vocab_size = len(self.token2idx)\n",
    "        self.input = Input(shape=(self.len_max, ), dtype='int32')\n",
    "        self.output = Embedding(input_dim=self.vocab_size,\n",
    "                                output_dim=self.embed_size,\n",
    "                                input_length=self.len_max,\n",
    "                                trainable=self.trainable,\n",
    "                                )(self.input)\n",
    "        self.model = Model(inputs=self.input, outputs=self.output)\n",
    "    \n",
    "    def sentence2idx(self, text):\n",
    "        text = self.extract_chinese(str(text)).upper()\n",
    "        text = list(text)\n",
    "        text = [text_one for text_one in text]\n",
    "        len_leave = self.len_max - len(text)\n",
    "\n",
    "        # 转换和填充处理\n",
    "        if len_leave >= 0:\n",
    "            text_index = [self.token2idx[text_char] if text_char in self.token2idx else self.token2idx['[UNK]'] for text_char in text] + [self.token2idx['[PAD]'] for i in range(len_leave)]\n",
    "        else:\n",
    "            text_index = [self.token2idx[text_char] if text_char in self.token2idx else self.token2idx['[UNK]'] for\n",
    "                          text_char in text[0:self.len_max]]\n",
    "        return text_index\n",
    "    \n",
    "    def idx2sentence(self, idx):\n",
    "        assert type(idx) == list\n",
    "        text_idx = [self.idx2token[id] if id in self.idx2token else self.idx2token['[UNK]'] for id in idx]\n",
    "        return \"\".join(text_idx)\n",
    "    \n",
    "    def extract_chinese(self, text):\n",
    "        \"\"\"\n",
    "              只提取出中文、字母和数字\n",
    "            :param text: str, input of sentence\n",
    "            :return:\n",
    "            \"\"\"\n",
    "        chinese_exttract = ''.join(re.findall(u\"([\\u4e00-\\u9fa5A-Za-z0-9@._])\", text))\n",
    "        return chinese_exttract   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 50)\n",
      "[[ 527  140  140  455   62 1429    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]\n",
      " [ 248  140  140  455  170   62 1429    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]]\n",
      "(2, 50, 300)\n",
      "[[[ 0.04584242  0.03654362 -0.01917813 ... -0.01199863 -0.01021879\n",
      "    0.03497038]\n",
      "  [ 0.02573839 -0.00362322  0.02442862 ... -0.01341845  0.0128829\n",
      "    0.03542539]\n",
      "  [ 0.02573839 -0.00362322  0.02442862 ... -0.01341845  0.0128829\n",
      "    0.03542539]\n",
      "  ...\n",
      "  [ 0.00539257  0.02650164 -0.003124   ...  0.03868828 -0.03746802\n",
      "   -0.0148697 ]\n",
      "  [ 0.00539257  0.02650164 -0.003124   ...  0.03868828 -0.03746802\n",
      "   -0.0148697 ]\n",
      "  [ 0.00539257  0.02650164 -0.003124   ...  0.03868828 -0.03746802\n",
      "   -0.0148697 ]]\n",
      "\n",
      " [[-0.0048676   0.03731116 -0.04381787 ... -0.00287968  0.03601387\n",
      "   -0.03165411]\n",
      "  [ 0.02573839 -0.00362322  0.02442862 ... -0.01341845  0.0128829\n",
      "    0.03542539]\n",
      "  [ 0.02573839 -0.00362322  0.02442862 ... -0.01341845  0.0128829\n",
      "    0.03542539]\n",
      "  ...\n",
      "  [ 0.00539257  0.02650164 -0.003124   ...  0.03868828 -0.03746802\n",
      "   -0.0148697 ]\n",
      "  [ 0.00539257  0.02650164 -0.003124   ...  0.03868828 -0.03746802\n",
      "   -0.0148697 ]\n",
      "  [ 0.00539257  0.02650164 -0.003124   ...  0.03868828 -0.03746802\n",
      "   -0.0148697 ]]]\n",
      "(2, 50, 300)\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "\n",
    "texts = [\"今天天气不错\",\n",
    "                 \"明天天气也不错\"]\n",
    "eb = RandomEmbedding()\n",
    "x = []\n",
    "for t in texts:\n",
    "    x.append(eb.sentence2idx(t))\n",
    "x = np.array(x)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "model = eb.model\n",
    "p = model.predict(x)\n",
    "print(p.shape)\n",
    "print(p)\n",
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
