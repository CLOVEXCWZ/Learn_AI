{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding XLNET\n",
    "\n",
    "**摘要：首先呢，需要承认一件事情，就是我还没有对XLNET去很好的了解=====。XLNET也是基于transfromer网络的，是google在推出BERT后再推出的网络，性能比BERT会更好。由于XLNET也是语言模型，所以也是可以做文本向量化的,本文采用XLNET中文预训练模型(chinese_xlnet_mid_L-24_H-768_A-12)进行向量化**\n",
    "\n",
    "[参考源码地址==========](https://github.com/yongzhuo/Keras-TextClassification)\n",
    "\n",
    "- 预训练模型说明\n",
    "    - 来源：哈工大进行中文预训练的\n",
    "    - [github原地址======](https://github.com/ymcui/Chinese-PreTrained-XLNet)\n",
    "    - 网络结构：XLNet-mid：24-layer, 768-hidden, 12-heads, 209M parameters\n",
    "    - 下载tensorflow版本(具体请打开链接查看)\n",
    "    \n",
    "XLNET以后会去探索，暂时呢就不知道了-------\n",
    "不过呢，不知道也可以先用着嘛=======【大笑】\n",
    "\n",
    "这里记录下今天（2019-11-27）探索BXLNET Embedding的实现代码，以便于以后有需要的时候能快速实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Add, Embedding\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.models import Input, Model\n",
    "\n",
    "import numpy as np\n",
    "import codecs\n",
    "import os\n",
    "import re\n",
    "\n",
    "path_embedding_xlnet = \"/Users/zhouwencheng/Desktop/Grass/data/model\" \\\n",
    "                       \"/ImportModel/XLNET/chinese_xlnet_mid_L-24_H-768_A-12\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from keras.engine import Layer\n",
    "\n",
    "\n",
    "class NonMaskingLayer(Layer):\n",
    "    \"\"\"\n",
    "    fix convolutional 1D can't receive masked input, detail: https://github.com/keras-team/keras/issues/4978\n",
    "    thanks for https://github.com/jacoxu\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        super(NonMaskingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XlnetEmbedding(object):\n",
    "    def __init__(self,\n",
    "                 len_max=50,  # 文本最大长度, 建议25-50\n",
    "                 embed_size=300,  # 嵌入层尺寸\n",
    "                 vocab_size=30000,  # 字典大小, 这里随便填的，会根据代码里修改\n",
    "                 trainable=False,  # 是否训练参数\n",
    "                 path_mode=path_embedding_xlnet,\n",
    "                 layer_indexes=[24], # 默认取最后一层的输出 大于13则取最后一层的输出\n",
    "                 xlnet_embed={},\n",
    "                 batch_size=2\n",
    "                ):\n",
    "        self.len_max = len_max\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.trainable = trainable \n",
    "        self.corpus_path = path_mode\n",
    "        self.layer_indexes = layer_indexes\n",
    "        self.xlnet_embed = xlnet_embed\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.model = None \n",
    "        self.build()\n",
    "        \n",
    " \n",
    "    def build(self):\n",
    "        from keras_xlnet import Tokenizer, ATTENTION_TYPE_BI, ATTENTION_TYPE_UNI\n",
    "        from keras_xlnet import load_trained_model_from_checkpoint\n",
    "\n",
    "        self.checkpoint_path = os.path.join(self.corpus_path, 'xlnet_model.ckpt')\n",
    "        self.config_path = os.path.join(self.corpus_path, 'xlnet_config.json')\n",
    "        self.spiece_model = os.path.join(self.corpus_path, 'spiece.model')\n",
    "\n",
    "        self.attention_type = 'bi'\n",
    "        self.attention_type = ATTENTION_TYPE_BI if self.attention_type == 'bi' else ATTENTION_TYPE_UNI\n",
    "        self.memory_len = 0\n",
    "        self.target_len = 5\n",
    "        print('load xlnet model start!')\n",
    "        model = load_trained_model_from_checkpoint(checkpoint_path=self.checkpoint_path,\n",
    "                                                   attention_type=self.attention_type,\n",
    "                                                   in_train_phase=self.trainable,\n",
    "                                                   config_path=self.config_path,\n",
    "                                                   memory_len=self.memory_len,\n",
    "                                                   target_len=self.target_len,\n",
    "                                                   batch_size=self.batch_size,\n",
    "                                                   mask_index=0)\n",
    "#         model.summary()\n",
    "        print('load xlnet model finish!')\n",
    "        # 字典加载\n",
    "        self.tokenizer = Tokenizer(self.spiece_model)\n",
    "        self.model_layers = model.layers\n",
    "        len_layers = self.model_layers.__len__()\n",
    "        print(len_layers)\n",
    "        len_couche = int((len_layers - 6) / 10)\n",
    "        # 一共246个layer\n",
    "        # 每层10个layer（MultiHeadAttention,Dropout,Add,LayerNormalization）,第一是9个layer的输入和embedding层\n",
    "        # 一共24层\n",
    "        layer_dict = [5]\n",
    "        layer_0 = 6\n",
    "        for i in range(len_couche):\n",
    "            layer_0 = layer_0 + 10\n",
    "            layer_dict.append(layer_0-2)\n",
    "        # 输出它本身\n",
    "        if len(self.layer_indexes) == 0:\n",
    "            encoder_layer = model.output\n",
    "            # 分类如果只有一层，取得不正确的话就取倒数第二层\n",
    "        elif len(self.layer_indexes) == 1:\n",
    "            if self.layer_indexes[0] in [i + 1 for i in range(len_couche + 1)]:\n",
    "                encoder_layer = model.get_layer(index=layer_dict[self.layer_indexes[0]]).output\n",
    "            else:\n",
    "                encoder_layer = model.get_layer(index=layer_dict[-1]).output\n",
    "        # 否则遍历需要取的层，把所有层的weight取出来并加起来shape:768*层数\n",
    "        else:\n",
    "            # layer_indexes must be [0, 1, 2,3,......24]\n",
    "            all_layers = [model.get_layer(index=layer_dict[lay]).output\n",
    "                          if lay in [i + 1 for i in range(len_couche + 1)]\n",
    "                          else model.get_layer(index=layer_dict[-1]).output  # 如果给出不正确，就默认输出倒数第一层\n",
    "                          for lay in self.layer_indexes]\n",
    "            print(self.layer_indexes)\n",
    "            print(all_layers)\n",
    "            all_layers_select = []\n",
    "            for all_layers_one in all_layers:\n",
    "                all_layers_select.append(all_layers_one)\n",
    "            encoder_layer = Add()(all_layers_select)\n",
    "            print(encoder_layer.shape)\n",
    "        self.output = NonMaskingLayer()(encoder_layer)\n",
    "        self.input = model.inputs\n",
    "        self.model = Model(inputs=self.input, outputs=self.output)\n",
    "\n",
    "        self.embedding_size = self.model.output_shape[-1]\n",
    "        self.vocab_size = len(self.tokenizer.sp)\n",
    "\n",
    "    def sentence2idx(self, text):\n",
    "        text = self.extract_chinese(str(text).upper())\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        tokens = tokens + [0] * (self.target_len - len(tokens)) \\\n",
    "            if len(tokens) < self.target_len \\\n",
    "            else tokens[0:self.target_len]\n",
    "        token_input = np.expand_dims(np.array(tokens), axis=0)\n",
    "        segment_input = np.zeros_like(token_input)\n",
    "        memory_length_input = np.zeros((1, 1))\n",
    "        return [token_input, segment_input, memory_length_input]\n",
    "    \n",
    "    \n",
    "    def extract_chinese(self, text):\n",
    "        \"\"\"\n",
    "          只提取出中文、字母和数字\n",
    "        :param text: str, input of sentence\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        chinese_exttract = ''.join(re.findall(u\"([\\u4e00-\\u9fa5A-Za-z0-9@._])\", text))\n",
    "        return chinese_exttract   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load xlnet model start!\n",
      "WARNING:tensorflow:From /Users/zhouwencheng/Desktop/Grass/101PythonEnv/envpy3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zhouwencheng/Desktop/Grass/101PythonEnv/envpy3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zhouwencheng/Desktop/Grass/101PythonEnv/envpy3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zhouwencheng/Desktop/Grass/101PythonEnv/envpy3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zhouwencheng/Desktop/Grass/101PythonEnv/envpy3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "load xlnet model finish!\n",
      "126\n",
      "(2, 5)\n",
      "(2, 5)\n",
      "(2, 1)\n",
      "[[   19  4328 10022    63  4856]\n",
      " [ 7350   118 10022  4180  4856]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[[0.]\n",
      " [0.]]\n",
      "(2, 5, 768)\n",
      "[[[-0.01283701  1.0316377   0.6240789  ... -0.25227237 -0.38330236\n",
      "   -0.26968622]\n",
      "  [ 0.11844011  1.0428874  -0.1372549  ... -0.47099146 -0.16267313\n",
      "   -0.01897161]\n",
      "  [ 0.7833793   0.7908515  -0.07405517 ... -0.11398397 -0.38520646\n",
      "    0.34588325]\n",
      "  [ 0.9593434   0.8058133   0.6556412  ... -0.4739001  -0.13973673\n",
      "    0.4029246 ]\n",
      "  [ 0.33478862  0.13342354  1.1380457  ... -1.1246004   1.0664203\n",
      "    0.34830493]]\n",
      "\n",
      " [[-0.8383907   1.1733837   0.07351154 ... -0.7073219  -0.21703273\n",
      "    0.32910395]\n",
      "  [-0.8203141   1.302698   -0.3939988  ... -0.6257708  -0.29780596\n",
      "    0.27401137]\n",
      "  [-0.6780075   0.5064006  -0.47468746 ... -0.31255782 -0.42654335\n",
      "    0.7236039 ]\n",
      "  [-0.6743382   0.04179962  0.9269874  ... -0.5992737  -0.8316488\n",
      "    1.5014975 ]\n",
      "  [-0.23544422 -0.9687195   1.2791495  ... -1.1714749   0.5062436\n",
      "    1.3038561 ]]]\n",
      "(2, 5, 768)\n"
     ]
    }
   ],
   "source": [
    "texts = [\"今天天气不错\",\n",
    "             \"明天天气也不错\"]\n",
    "eb = XlnetEmbedding()\n",
    "x = []\n",
    "x_segment = []\n",
    "x_memory = []\n",
    "for t in texts:\n",
    "    x_buff, x_segment_buff, x_memory_buff = eb.sentence2idx(t)\n",
    "    x.append(x_buff[0])\n",
    "    x_segment.append(x_segment_buff[0])\n",
    "    x_memory.append(x_memory_buff[0])\n",
    "\n",
    "x = np.array(x)\n",
    "x_segment = np.array(x_segment)\n",
    "x_memory = np.array(x_memory)\n",
    "\n",
    "print(x.shape)\n",
    "print(x_segment.shape)\n",
    "print(x_memory.shape)\n",
    "\n",
    "print(x)\n",
    "print(x_segment)\n",
    "print(x_memory)\n",
    "\n",
    "model = eb.model\n",
    "p = model.predict([x, x_segment, x_memory])\n",
    "print(p.shape)\n",
    "print(p)\n",
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
