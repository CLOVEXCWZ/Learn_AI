{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding BERT\n",
    "\n",
    "**摘要: BERT的出现是令人兴奋的，它足以方NLP走上一个台阶。BERT作为语言模型在它出现的时候表现可谓是惊艳，在一段时间内统领NLP的多个领域，它的预训练模型便是促使这一些的主要原因。BERT本质是一个语言模型，于是用它来做向量化也就是常理之中了。本次探索加载预训练模型(chinese_L-12_H-768_A-12)进行文字的向量化**\n",
    "\n",
    "[参考源码地址==========](https://github.com/yongzhuo/Keras-TextClassification)\n",
    "\n",
    "BERT的强大就不做赘述了，不过BERT需要耗费的算力也是非常让人头疼的事情了！也至于我知道现在有也没有能真正去好好是使用BERT等一些比较大型的网络，是在是有些遗憾。等哥以后有GPU了(可以用到GPU资源，不是说自己买哈-------=-=-=)哥一定好好去跑这些大型网络，想想还有点开心。\n",
    "\n",
    "这里记录下今天（2019-11-27）探索BERT Embedding的实现代码，以便于以后有需要的时候能快速实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Add, Embedding\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.models import Input, Model\n",
    "\n",
    "import numpy as np\n",
    "import codecs\n",
    "import os\n",
    "import re\n",
    "\n",
    "path_embedding_bert = \"/Users/zhouwencheng/Desktop/Grass/data/model\" \\\n",
    "                      \"/ImportModel/BERT/chinese_L-12_H-768_A-12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from keras.engine import Layer\n",
    "\n",
    "\n",
    "class NonMaskingLayer(Layer):\n",
    "    \"\"\"\n",
    "    fix convolutional 1D can't receive masked input, detail: https://github.com/keras-team/keras/issues/4978\n",
    "    thanks for https://github.com/jacoxu\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        super(NonMaskingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbedding(object):\n",
    "    def __init__(self,\n",
    "                 len_max=50,  # 文本最大长度, 建议25-50\n",
    "                 embed_size=300,  # 嵌入层尺寸\n",
    "                 vocab_size=30000,  # 字典大小, 这里随便填的，会根据代码里修改\n",
    "                 trainable=True,  # 是否训练参数\n",
    "                 path_mode=path_embedding_bert,\n",
    "                 layer_indexes=[24] # 默认取最后一层的输出 大于13则取最后一层的输出\n",
    "                ):\n",
    "        self.len_max = len_max\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.trainable = trainable \n",
    "        self.path_mode = path_mode\n",
    "        self.layer_indexes = layer_indexes\n",
    "        \n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.model = None \n",
    "        self.build()\n",
    "        \n",
    " \n",
    "\n",
    "    def build(self):\n",
    "        import keras_bert\n",
    "\n",
    "        config_path = os.path.join(self.path_mode, 'bert_config.json')\n",
    "        check_point_path = os.path.join(self.path_mode, 'bert_model.ckpt')\n",
    "        dict_path = os.path.join(self.path_mode, 'vocab.txt')\n",
    "        print('load bert model start!')\n",
    "        model = keras_bert.load_trained_model_from_checkpoint(config_path,\n",
    "                                                              checkpoint_file=check_point_path,\n",
    "                                                              seq_len=self.len_max,\n",
    "                                                              trainable=self.trainable)\n",
    "        print('load bert model end!')\n",
    "\n",
    "        layer_dict = [6]\n",
    "        layer_0 = 7\n",
    "        for i in range(12):\n",
    "            layer_0 = layer_0 + 8\n",
    "            layer_dict.append(layer_0)\n",
    "        print(layer_dict)\n",
    "\n",
    "        # 输出他本身\n",
    "        if len(self.layer_indexes) == 0:\n",
    "            encoder_layer = model.output\n",
    "        # 分类如果只有一层，就只取最后那一层的weight；取得不正确，就默认取最后一层\n",
    "        elif len(self.layer_indexes) == 1:\n",
    "            if self.layer_indexes[0] in [i + 1 for i in range(13)]:\n",
    "                encoder_layer = model.get_layer(index=layer_dict[self.layer_indexes[0]-1]).output\n",
    "            else:\n",
    "                encoder_layer = model.get_layer(index=layer_dict[-1]).output\n",
    "        # 否则遍历需要取的层，把所有层的weight取出来并拼接起来shape:768*层数\n",
    "        else:\n",
    "            # layer_indexes must be [1,2,3,......12]\n",
    "            # all_layers = [model.get_layer(index=lay).output if lay is not 1 else model.get_layer(index=lay).output[0] for lay in layer_indexes]\n",
    "            all_layers = [model.get_layer(index=layer_dict[lay - 1]).output if lay in [i + 1 for i in range(13)]\n",
    "                          else model.get_layer(index=layer_dict[-1]).output  # 如果给出不正确，就默认输出最后一层\n",
    "                          for lay in self.layer_indexes]\n",
    "            all_layers_select = []\n",
    "            for all_layers_one in all_layers:\n",
    "                all_layers_select.append(all_layers_one)\n",
    "            encoder_layer = Add()(all_layers_select)\n",
    "        self.output = NonMaskingLayer()(encoder_layer)\n",
    "        self.input = model.inputs\n",
    "        self.model = Model(inputs=self.input, outputs=self.output)\n",
    "        self.embedding_size = self.model.output_shape[-1]\n",
    "\n",
    "        self.token_dict = {}\n",
    "        with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
    "            for line in reader:\n",
    "                token = line.strip()\n",
    "                self.token_dict[token] = len(self.token_dict)\n",
    "        self.vocab_size = len(self.token_dict)\n",
    "        self.tokenizer = keras_bert.Tokenizer(self.token_dict)\n",
    "\n",
    "    \n",
    "    def sentence2idx(self, text, second_text=None):\n",
    "        text = self.extract_chinese(str(text)).upper()\n",
    "        input_id, input_type_id = self.tokenizer.encode(first=text,\n",
    "                                                        second=second_text,\n",
    "                                                        max_len=self.len_max)\n",
    "        return [input_id, input_type_id]\n",
    "    \n",
    "    \n",
    "    def extract_chinese(self, text):\n",
    "        \"\"\"\n",
    "          只提取出中文、字母和数字\n",
    "        :param text: str, input of sentence\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        chinese_exttract = ''.join(re.findall(u\"([\\u4e00-\\u9fa5A-Za-z0-9@._])\", text))\n",
    "        return chinese_exttract   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load bert model start!\n",
      "WARNING:tensorflow:From /Users/zhouwencheng/Desktop/Grass/101PythonEnv/envpy3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zhouwencheng/Desktop/Grass/101PythonEnv/envpy3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zhouwencheng/Desktop/Grass/101PythonEnv/envpy3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zhouwencheng/Desktop/Grass/101PythonEnv/envpy3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/zhouwencheng/Desktop/Grass/101PythonEnv/envpy3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/zhouwencheng/Desktop/Grass/101PythonEnv/envpy3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "load bert model end!\n",
      "[6, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95, 103]\n",
      "(2, 50)\n",
      "(2, 50)\n",
      "(2, 50, 768)\n",
      "[[[-0.2086307   0.35827795  0.21853793 ... -0.44025412  0.4094602\n",
      "   -0.17016348]\n",
      "  [-0.39738995  0.36013675  0.7867802  ... -1.0404619  -0.18768609\n",
      "   -0.14245623]\n",
      "  [ 0.30535394 -0.4651899  -0.37177834 ...  0.28120545  0.8466301\n",
      "    0.05893405]\n",
      "  ...\n",
      "  [ 0.46516663 -0.4488659  -0.32406497 ...  0.310403   -0.03735422\n",
      "   -0.20383228]\n",
      "  [ 0.40643167 -0.45044908  0.05603563 ...  0.08266892 -0.00854655\n",
      "   -0.45705166]\n",
      "  [ 0.24296917  0.0176946  -0.1882883  ...  0.00265872 -0.22536862\n",
      "   -0.23967592]]\n",
      "\n",
      " [[-0.07990564  0.23048158  0.50818104 ... -0.60418904  0.3972589\n",
      "   -0.3813861 ]\n",
      "  [-0.44916582  0.0599215   0.53094774 ... -0.83169353 -0.35385585\n",
      "    0.20767196]\n",
      "  [-0.11684791 -0.95862097 -0.12063565 ...  0.9308959   0.7715939\n",
      "    0.07150119]\n",
      "  ...\n",
      "  [ 0.1459713  -0.45389768 -0.15324628 ...  0.18871553  0.05846509\n",
      "   -0.21455078]\n",
      "  [ 0.16451004 -0.38668138 -0.03275665 ... -0.0301722  -0.10589089\n",
      "   -0.4681597 ]\n",
      "  [ 0.19210465 -0.33747566 -0.0845611  ... -0.01139551  0.09283289\n",
      "   -0.38257232]]]\n",
      "(2, 50, 768)\n"
     ]
    }
   ],
   "source": [
    "texts = [\"今天天气不错\",\n",
    "             \"明天天气也不错\"]\n",
    "eb = BertEmbedding()\n",
    "x = []\n",
    "x_type = []\n",
    "for t in texts:\n",
    "    x_buff, x_type_buff = eb.sentence2idx(t)\n",
    "    x.append(x_buff)\n",
    "    x_type.append(x_type_buff)\n",
    "x = np.array(x)\n",
    "x_type = np.array(x_type)\n",
    "\n",
    "print(x.shape)\n",
    "print(x_type.shape)\n",
    "\n",
    "model = eb.model\n",
    "p = model.predict([x, x_type])\n",
    "print(p.shape)\n",
    "print(p)\n",
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
