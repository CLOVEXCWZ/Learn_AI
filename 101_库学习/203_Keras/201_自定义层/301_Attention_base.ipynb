{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 网页地址: https://blog.csdn.net/uhauha2929/article/details/80733255\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    " \n",
    "from keras.engine.base_layer import InputSpec\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        # W.shape = (time_steps, time_steps)\n",
    "        self.W = self.add_weight(name='att_weight',\n",
    "                                shape=(input_shape[1], input_shape[1]),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\",\n",
    "                                shape=(input_shape[1],),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # input.shape = (batch_size, time_steps, seq_len)\n",
    "        x = K.permute_dimensions(inputs, (0, 2, 1))\n",
    "        # x.shape = (batch_size, seq_len, time_steps)\n",
    "        a = K.softmax(K.tanh(K.dot(x, self.W) + self.b))\n",
    "        outputs = K.permute_dimensions(a*x, (0, 2, 1))\n",
    "        outputs = K.sum(outputs, axis=1)\n",
    "        return outputs\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[2]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "\t'''\n",
    "\t\t返回值：\n",
    "\t\t\t返回的不是attention权重，而是每个timestep乘以权重后相加得到的向量。\n",
    "\t\t输入:\n",
    "\t\t\t输入是rnn的timesteps，也是最长输入序列的长度。keras\n",
    "\t'''\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "    \t# 后面的层不需要mask了，所以这里可以直接返回none\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "\t\t# 这里应该是 step_dim是我们指定的参数，它等于input_shape[1],也就是rnn的timesteps\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "\t\t# 输入和参数分别reshape再点乘后，tensor.shape变成了(batch_size*timesteps, 1),之后每个batch要分开进行归一化\n",
    "\t\t# 所以应该有 eij = K.reshape(..., (-1, timesteps))\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\t\t# RNN一般默认激活函数为tanh, 对attention来说激活函数差别不打，因为要做softmax\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "        # 如果前面的层有mask，那么后面这些被mask掉的timestep肯定是不能参与计算输出的，也就是将他们的attention权重设为0\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\t\t# cast是做类型转换，keras计算时会检查类型，可能是因为用gpu的原因\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "\t\t# a = K.expand_dims(a, axis=-1) , axis默认为-1， 表示在最后扩充一个维度。\n",
    "\t\t# 比如shape = (3,)变成 (3, 1)\n",
    "        a = K.expand_dims(a)\n",
    "        # 此时a.shape = (batch_size, timesteps, 1), x.shape = (batch_size, timesteps, units)\n",
    "        weighted_input = x * a\n",
    "\n",
    "\t\t# weighted_input的shape为 (batch_size, timesteps, units), 每个timestep的输出向量已经乘上了该timestep的权重\n",
    "\t\t# weighted_input在axis=1上取和，返回值的shape为 (batch_size, 1, units)\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "    \t# 返回的结果是c，其shape为 (batch_size, units)\n",
    "        return input_shape[0],  self.features_dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
