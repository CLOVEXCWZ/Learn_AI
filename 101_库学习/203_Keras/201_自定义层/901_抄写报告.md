<font size=5> **2019-11-25 Layer的抄写小结**</font>



**摘要**：keras的Layer是非常重要的一部分，它是神经网络的基本构成，想要好好理解keras的神经网络就需要好好理解keras。抄写是我理解这些Layer的一个基本而又非常重要的手段，这样能加快我理解Layer同时了解里面的构造。今天抄写了Transformer的FeedForward、TokenEmbedding、EmbeddingSimilarity(EmbeddingRet、EmbeddingSim)，以及keras的源码Embedding、LayerNormalization、Masking、Dropout、Activation、Reshape、Permute、Flatten、RepeatVector、Dense、ActivityRegularization。



**缘由**

​	在Bert网络中看到很多层都是需要自己去定义的，而不是keras官方包里面定义的，在看源码的时候感觉看得似懂非懂，而这些又是代码中最重要的一部分，就觉得如果需要能看懂甚至需要自己去实现这些东西的时候就需要真正的去理解去弄明白这些东西的原理。

​	因为不仅仅的Bert的这些层需要自己去实现，在之前的Attention层等就需要自己去实现，所以如果要能真正去理解这些自己实现的层，就需要真正明白Layer的整个流程。

​	想到要一下子就把Layer弄懂这是不现实的，于是呢我就采用一个最笨同时也是最省事的办法，就是抄写，这样既可以看到里面的构成，同时可以不用太费力气去想事情。

​	理解这些Layer的目的有两个，一个是能看懂这些Layer的构成和原理，第二是为以后自己编写一些Layer打下基础。



**过程**

​	今天从上午开始，就开始抄写Bert的一些层，下午和晚上主要抄写keras源码里面的一些层。

​	整体感觉不错，还没到能实现一个Layer的程度，但基本都可以理解每个Layer（比较简单的Layer）在做什么，已经达到了初步的目的，接下来就需要在学习的过程中去实践去应用，在实践中提高自己了。





PS；今天估计是中午没有午休的原因，导致下午下班后整个人比较疲惫  包括现在也是比较疲惫-----以后还是需要好好午休呀--- 加油吧 小伙子













