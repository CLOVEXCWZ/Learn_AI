{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import recurrentshop\n",
    "from recurrentshop.cells import *\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, Activation\n",
    "from keras.layers import add, multiply, concatenate\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoderCell(ExtendedRNNCell):\n",
    "    def __init__(self, hidden_dim=None, **kwargs):\n",
    "        if hidden_dim:\n",
    "            self.hidden_dim = hidden_dim\n",
    "        else:\n",
    "            self.hidden_dim = self.output_dim\n",
    "        super(LSTMDecoderCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build_model(self, input_shape):\n",
    "        hidden_dim = self.hidden_dim\n",
    "        output_dim = self.output_dim\n",
    "        x = Input(batch_shape=input_shape)\n",
    "        h_tm1 = Input(batch_shape=(input_shape[0], hidden_dim))\n",
    "        c_tm1 = Input(batch_shape=(input_shape[0], hidden_dim))\n",
    "\n",
    "        W1 = Dense(hidden_dim * 4,\n",
    "                   kernel_initializer=self.kernel_initializer,\n",
    "                   kernel_regularizer=self.kernel_regularizer,\n",
    "                   use_bias=False)\n",
    "        W2 = Dense(output_dim,\n",
    "                   kernel_initializer=self.kernel_initializer,\n",
    "                   kernel_regularizer=self.kernel_regularizer,)\n",
    "        U = Dense(hidden_dim * 4,\n",
    "                  kernel_initializer=self.kernel_initializer,\n",
    "                  kernel_regularizer=self.kernel_regularizer,)\n",
    "        z = add([W1(x), U(h_tm1)])\n",
    "        z0, z1, z2, z3 = get_slices(z, 4)\n",
    "        i = Activation(self.recurrent_activation)(z0)\n",
    "        f = Activation(self.recurrent_activation)(z1)\n",
    "        c = add([multiply([f, c_tm1]), multiply([i, Activation(self.activation)(z2)])])\n",
    "        o = Activation(self.recurrent_activation)(z3)\n",
    "        h = multiply([o, Activation(self.activation)(c)])\n",
    "        y = Activation(self.activation)(W2(h))\n",
    "        return Model([x, h_tm1, c_tm1], [y, h, c])\n",
    "\n",
    "\n",
    "class AttentionDecoderCell(ExtendedRNNCell):\n",
    "    def __init__(self, hidden_dim=None, **kwargs):\n",
    "        if hidden_dim:\n",
    "            self.hidden_dim = hidden_dim\n",
    "        else:\n",
    "            self.hidden_dim = self.output_dim\n",
    "        self.input_ndim = 3\n",
    "        super(AttentionDecoderCell, self).__init__(**kwargs)\n",
    "    def build_model(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        output_dim = self.output_dim\n",
    "        input_length = input_shape[1]\n",
    "        hidden_dim = self.hidden_dim\n",
    "        x = Input(batch_shape=input_shape)\n",
    "        h_tm1 = Input(batch_shape=(input_shape[0], hidden_dim))\n",
    "        c_tm1 = Input(batch_shape=(input_shape[0], hidden_dim))\n",
    "        W1 = Dense(hidden_dim * 4,\n",
    "                   kernel_initializer=self.kernel_initializer,\n",
    "                   kernel_regularizer=self.kernel_regularizer)\n",
    "        W2 = Dense(output_dim,\n",
    "                   kernel_initializer=self.kernel_initializer,\n",
    "                   kernel_regularizer=self.kernel_regularizer)\n",
    "        W3 = Dense(1,\n",
    "                   kernel_initializer=self.kernel_initializer,\n",
    "                   kernel_regularizer=self.kernel_regularizer)\n",
    "        U = Dense(hidden_dim * 4,\n",
    "                  kernel_initializer=self.kernel_initializer,\n",
    "                  kernel_regularizer=self.kernel_regularizer)\n",
    "        C = Lambda(lambda x: K.repeat(x, input_length), output_shape=(input_length, input_dim))(c_tm1)\n",
    "        _xC = concatenate([x, C])\n",
    "        _xC = Lambda(lambda x: K.reshape(x, (-1, input_dim + hidden_dim)), output_shape=(input_dim + hidden_dim,))(_xC)\n",
    "        alpha = W3(_xC)\n",
    "        alpha = Lambda(lambda x: K.reshape(x, (-1, input_length)), output_shape=(input_length,))(alpha)\n",
    "        alpha = Activation('softmax')(alpha)\n",
    "        _x = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=(1, 1)), output_shape=(input_dim,))([alpha, x])\n",
    "        z = add([W1(_x), U(h_tm1)])\n",
    "        z0, z1, z2, z3 = get_slices(z, 4)\n",
    "        i = Activation(self.recurrent_activation)(z0)\n",
    "        f = Activation(self.recurrent_activation)(z1)\n",
    "        c = add([multiply([f, c_tm1]), multiply([i, Activation(self.activation)(z2)])])\n",
    "        o = Activation(self.recurrent_activation)(z3)\n",
    "        h = multiply([o, Activation(self.activation)(c)])\n",
    "        y = Activation(self.activation)(W2(h))\n",
    "        return Model([x, h_tm1, c_tm1], [y, h, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from recurrentshop import LSTMCell, RecurrentSequential \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, TimeDistributed, Bidirectional, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_479 (InputLayer)          (None, 500, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "recurrent_sequential_17 (Recurr (None, 128)          219648      input_479[0][0]                  \n",
      "                                                                 private__optional_input_place_hol\n",
      "                                                                 private__optional_input_place_hol\n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "recurrent_sequential_18 (Recurr (None, 600, 300)     1236000     recurrent_sequential_17[0][0]    \n",
      "                                                                 private__optional_input_place_hol\n",
      "                                                                 private__optional_input_place_hol\n",
      "                                                                 private__optional_input_place_hol\n",
      "                                                                 private__optional_input_place_hol\n",
      "==================================================================================================\n",
      "Total params: 1,455,648\n",
      "Trainable params: 1,455,648\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = SimpleSeq2Seq(output_dim=300, hidden_dim=128, output_length=600, input_shape=(500, 300))\n",
    "# 把模型保存为图片\n",
    "from keras.utils import plot_model\n",
    "plot_model(model3,to_file='model_png/201_3SimpleSeq2seq.png', show_layer_names=True, show_shapes=True) \n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimpleSeq2Seq(output_dim, output_length, hidden_dim=None, input_shape=None,\n",
    "                  batch_size=None, batch_input_shape=None, input_dim=None,\n",
    "                  input_length=None, depth=1, dropout=0.0, unroll=False,\n",
    "                  stateful=False):\n",
    "    if isinstance(depth, int):\n",
    "        depth = (depth, depth)\n",
    "    if batch_input_shape:\n",
    "        shape = batch_input_shape\n",
    "    elif input_shape:\n",
    "        shape = (batch_size,) + input_shape\n",
    "    elif input_dim:\n",
    "        if input_length:\n",
    "            shape = (batch_size,) + (input_length,) + (input_dim,)\n",
    "        else:\n",
    "            shape = (batch_size,) + (None,) + (input_dim,)\n",
    "    else:\n",
    "        # TODO Proper error message\n",
    "        raise TypeError\n",
    "    if hidden_dim is None:\n",
    "        hidden_dim = output_dim\n",
    "    encoder = RecurrentSequential(unroll=unroll, stateful=stateful)\n",
    "    encoder.add(LSTMCell(hidden_dim, batch_input_shape=(shape[0], shape[-1])))\n",
    "\n",
    "    for _ in range(1, depth[0]):\n",
    "        encoder.add(Dropout(dropout))\n",
    "        encoder.add(LSTMCell(hidden_dim))\n",
    "\n",
    "    decoder = RecurrentSequential(unroll=unroll, stateful=stateful,\n",
    "                                  decode=True, output_length=output_length)\n",
    "    decoder.add(Dropout(dropout, batch_input_shape=(shape[0], hidden_dim)))\n",
    "\n",
    "    if depth[1] == 1:\n",
    "        decoder.add(LSTMCell(output_dim))\n",
    "    else:\n",
    "        decoder.add(LSTMCell(hidden_dim))\n",
    "        for _ in range(depth[1] - 2):\n",
    "            decoder.add(Dropout(dropout))\n",
    "            decoder.add(LSTMCell(hidden_dim))\n",
    "    decoder.add(Dropout(dropout))\n",
    "    decoder.add(LSTMCell(output_dim))\n",
    "\n",
    "    _input = Input(batch_shape=shape)\n",
    "    x = encoder(_input)\n",
    "    output = decoder(x)\n",
    "    return Model(_input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_444 (InputLayer)          (None, 500, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 500, 128)     38528       input_444[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "recurrent_sequential_15 (Recurr [(None, 128), (None, 263168      time_distributed_2[0][0]         \n",
      "                                                                 private__optional_input_place_hol\n",
      "                                                                 private__optional_input_place_hol\n",
      "                                                                 private__optional_input_place_hol\n",
      "__________________________________________________________________________________________________\n",
      "dense_269 (Dense)               (None, 300)          38700       recurrent_sequential_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "input_460 (InputLayer)          (None, 500, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "recurrent_sequential_16 (Recurr (None, 500, 300)     516696      dense_269[0][0]                  \n",
      "                                                                 recurrent_sequential_15[0][1]    \n",
      "                                                                 recurrent_sequential_15[0][2]    \n",
      "                                                                 dense_269[0][0]                  \n",
      "                                                                 input_460[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 857,092\n",
      "Trainable params: 857,092\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Seq2Seq(output_dim=300, hidden_dim=128, output_length=500, input_shape=(500, 300), peek=True, depth=2, teacher_force=True)\n",
    "# 把模型保存为图片\n",
    "from keras.utils import plot_model\n",
    "plot_model(model2,to_file='model_png/201_2Seq2seq.png', show_layer_names=True, show_shapes=True) \n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Seq2Seq(output_dim, output_length, batch_input_shape=None,\n",
    "            input_shape=None, batch_size=None, input_dim=None, input_length=None,\n",
    "            hidden_dim=None, depth=1, broadcast_state=True, unroll=False,\n",
    "            stateful=False, inner_broadcast_state=True, teacher_force=False,\n",
    "            peek=False, dropout=0.):\n",
    "    if isinstance(depth, int):\n",
    "        depth = (depth, depth)\n",
    "    if batch_input_shape:\n",
    "        shape = batch_input_shape\n",
    "    elif input_shape:\n",
    "        shape = (batch_size,) + input_shape\n",
    "    elif input_dim:\n",
    "        if input_length:\n",
    "            shape = (batch_size,) + (input_length,) + (input_dim,)\n",
    "        else:\n",
    "            shape = (batch_size,) + (None,) + (input_dim,)\n",
    "    else: \n",
    "        raise TypeError\n",
    "    if hidden_dim is None:\n",
    "        hidden_dim = output_dim\n",
    "\n",
    "    encoder = RecurrentSequential(readout=True, state_sync=inner_broadcast_state,\n",
    "                                  unroll=unroll, stateful=stateful,\n",
    "                                  return_states=broadcast_state)\n",
    "    for _ in range(depth[0]):\n",
    "        encoder.add(LSTMCell(hidden_dim, batch_input_shape=(shape[0], hidden_dim)))\n",
    "        encoder.add(Dropout(dropout))\n",
    "\n",
    "    dense1 = TimeDistributed(Dense(hidden_dim))\n",
    "    dense1.supports_masking = True\n",
    "    dense2 = Dense(output_dim)\n",
    "\n",
    "    decoder = RecurrentSequential(readout='add' if peek else 'readout_only',\n",
    "                                  state_sync=inner_broadcast_state, decode=True,\n",
    "                                  output_length=output_length, unroll=unroll,\n",
    "                                  stateful=stateful, teacher_force=teacher_force)\n",
    "\n",
    "    for _ in range(depth[1]):\n",
    "        decoder.add(Dropout(dropout, batch_input_shape=(shape[0], output_dim)))\n",
    "        decoder.add(LSTMDecoderCell(output_dim=output_dim, hidden_dim=hidden_dim,\n",
    "                                    batch_input_shape=(shape[0], output_dim)))\n",
    "\n",
    "    _input = Input(batch_shape=shape)\n",
    "    _input._keras_history[0].supports_masking = True\n",
    "    encoded_seq = dense1(_input)\n",
    "    encoded_seq = encoder(encoded_seq)\n",
    "    if broadcast_state:\n",
    "        assert type(encoded_seq) is list\n",
    "        states = encoded_seq[-2:]\n",
    "        encoded_seq = encoded_seq[0]\n",
    "    else:\n",
    "        states = None\n",
    "    encoded_seq = dense2(encoded_seq)\n",
    "    inputs = [_input]\n",
    "    if teacher_force:\n",
    "        truth_tensor = Input(batch_shape=(shape[0], output_length, output_dim))\n",
    "        truth_tensor._keras_history[0].supports_masking = True\n",
    "        inputs += [truth_tensor]\n",
    "\n",
    "\n",
    "    decoded_seq = decoder(encoded_seq,\n",
    "                          ground_truth=inputs[1] if teacher_force else None,\n",
    "                          initial_readout=encoded_seq, initial_state=states)\n",
    "    \n",
    "    model = Model(inputs, decoded_seq)\n",
    "    model.encoder = encoder\n",
    "    model.decoder = decoder\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_336 (InputLayer)          (None, 10, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 10, 128)      439296      input_336[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "recurrent_sequential_12 (Recurr (None, 5000, 300)    171053      bidirectional_6[0][0]            \n",
      "                                                                 private__optional_input_place_hol\n",
      "                                                                 private__optional_input_place_hol\n",
      "==================================================================================================\n",
      "Total params: 610,349\n",
      "Trainable params: 610,349\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1=AttentionSeq2Seq(output_dim=300, hidden_dim=128, output_length=5000, input_shape=(10, 300), depth=1, dropout=0.2)\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把模型保存为图片\n",
    "from keras.utils import plot_model\n",
    "plot_model(model1,to_file='model_png/201grass1.png', show_layer_names=True, show_shapes=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AttentionSeq2Seq(output_dim, output_length, batch_input_shape=None,\n",
    "                     batch_size=None, input_shape=None, input_length=None,\n",
    "                     input_dim=None, hidden_dim=None, depth=1,\n",
    "                     bidirectional=True, unroll=False, stateful=False, dropout=0.0,):\n",
    "    if isinstance(depth, int):\n",
    "        depth = (depth, depth)\n",
    "    if batch_input_shape:\n",
    "        shape = batch_input_shape\n",
    "    elif input_shape:\n",
    "        shape = (batch_size,) + input_shape\n",
    "    elif input_dim:\n",
    "        if input_length:\n",
    "            shape = (batch_size,) + (input_length,) + (input_dim,)\n",
    "        else:\n",
    "            shape = (batch_size,) + (None,) + (input_dim,)\n",
    "    else: \n",
    "        raise TypeError\n",
    "    if hidden_dim is None:\n",
    "        hidden_dim = output_dim\n",
    "    _input = Input(batch_shape=shape)\n",
    "    _input._keras_history[0].supports_masking = True\n",
    "    encoder = RecurrentSequential(unroll=unroll, stateful=stateful,\n",
    "                                  return_sequences=True)\n",
    "    encoder.add(LSTMCell(hidden_dim, batch_input_shape=(shape[0], shape[2])))\n",
    "    for _ in range(1, depth[0]):\n",
    "        encoder.add(Dropout(dropout))\n",
    "        encoder.add(LSTMCell(hidden_dim))\n",
    "    if bidirectional:\n",
    "        encoder = Bidirectional(encoder, merge_mode='sum')\n",
    "        encoder.forward_layer.build(shape)\n",
    "        encoder.backward_layer.build(shape) \n",
    "        encoder.layer = encoder.forward_layer\n",
    "    encoded = encoder(_input)\n",
    "    decoder = RecurrentSequential(decode=True, output_length=output_length,\n",
    "                                  unroll=unroll, stateful=stateful)\n",
    "    decoder.add(Dropout(dropout, batch_input_shape=(shape[0], shape[1], hidden_dim)))\n",
    "    if depth[1] == 1:\n",
    "        decoder.add(AttentionDecoderCell(output_dim=output_dim, hidden_dim=hidden_dim))\n",
    "    else:\n",
    "        decoder.add(AttentionDecoderCell(output_dim=output_dim, hidden_dim=hidden_dim))\n",
    "        for _ in range(depth[1] - 2):\n",
    "            decoder.add(Dropout(dropout))\n",
    "            decoder.add(LSTMDecoderCell(output_dim=hidden_dim, hidden_dim=hidden_dim))\n",
    "        decoder.add(Dropout(dropout))\n",
    "        decoder.add(LSTMDecoderCell(output_dim=output_dim, hidden_dim=hidden_dim))\n",
    "    \n",
    "    inputs = [_input]\n",
    "    decoded = decoder(encoded)\n",
    "    model = Model(inputs, decoded)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
