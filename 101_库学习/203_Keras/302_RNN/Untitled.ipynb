{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recurrentshop.cells import *\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, Activation\n",
    "from keras.layers import add, multiply, concatenate\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoderCell(ExtendedRNNCell):\n",
    "\n",
    "    def __init__(self, hidden_dim=None, **kwargs):\n",
    "        if hidden_dim:\n",
    "            self.hidden_dim = hidden_dim\n",
    "        else:\n",
    "            self.hidden_dim = self.output_dim\n",
    "        super(LSTMDecoderCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build_model(self, input_shape):\n",
    "        hidden_dim = self.hidden_dim\n",
    "        output_dim = self.output_dim\n",
    "\n",
    "        x = Input(batch_shape=input_shape)\n",
    "        h_tm1 = Input(batch_shape=(input_shape[0], hidden_dim))\n",
    "        c_tm1 = Input(batch_shape=(input_shape[0], hidden_dim))\n",
    "\n",
    "        W1 = Dense(hidden_dim * 4,\n",
    "                   kernel_initializer=self.kernel_initializer,\n",
    "                   kernel_regularizer=self.kernel_regularizer,\n",
    "                   use_bias=False)\n",
    "        W2 = Dense(output_dim,\n",
    "                   kernel_initializer=self.kernel_initializer,\n",
    "                   kernel_regularizer=self.kernel_regularizer,)\n",
    "        U = Dense(hidden_dim * 4,\n",
    "                  kernel_initializer=self.kernel_initializer,\n",
    "                  kernel_regularizer=self.kernel_regularizer,)\n",
    "\n",
    "        z = add([W1(x), U(h_tm1)])\n",
    "\n",
    "        z0, z1, z2, z3 = get_slices(z, 4)\n",
    "        i = Activation(self.recurrent_activation)(z0)\n",
    "        f = Activation(self.recurrent_activation)(z1)\n",
    "        c = add([multiply([f, c_tm1]), multiply([i, Activation(self.activation)(z2)])])\n",
    "        o = Activation(self.recurrent_activation)(z3)\n",
    "        h = multiply([o, Activation(self.activation)(c)])\n",
    "        y = Activation(self.activation)(W2(h))\n",
    "\n",
    "        return Model([x, h_tm1, c_tm1], [y, h, c])\n",
    "\n",
    "\n",
    "class AttentionDecoderCell(ExtendedRNNCell):\n",
    "\n",
    "    def __init__(self, hidden_dim=None, **kwargs):\n",
    "        if hidden_dim:\n",
    "            self.hidden_dim = hidden_dim\n",
    "        else:\n",
    "            self.hidden_dim = self.output_dim\n",
    "        self.input_ndim = 3\n",
    "        super(AttentionDecoderCell, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def build_model(self, input_shape):\n",
    "        \n",
    "        input_dim = input_shape[-1]\n",
    "        output_dim = self.output_dim\n",
    "        input_length = input_shape[1]\n",
    "        hidden_dim = self.hidden_dim\n",
    "\n",
    "        x = Input(batch_shape=input_shape)\n",
    "        h_tm1 = Input(batch_shape=(input_shape[0], hidden_dim))\n",
    "        c_tm1 = Input(batch_shape=(input_shape[0], hidden_dim))\n",
    "        \n",
    "        W1 = Dense(hidden_dim * 4,\n",
    "                   kernel_initializer=self.kernel_initializer,\n",
    "                   kernel_regularizer=self.kernel_regularizer)\n",
    "        W2 = Dense(output_dim,\n",
    "                   kernel_initializer=self.kernel_initializer,\n",
    "                   kernel_regularizer=self.kernel_regularizer)\n",
    "        W3 = Dense(1,\n",
    "                   kernel_initializer=self.kernel_initializer,\n",
    "                   kernel_regularizer=self.kernel_regularizer)\n",
    "        U = Dense(hidden_dim * 4,\n",
    "                  kernel_initializer=self.kernel_initializer,\n",
    "                  kernel_regularizer=self.kernel_regularizer)\n",
    "\n",
    "        C = Lambda(lambda x: K.repeat(x, input_length), output_shape=(input_length, input_dim))(c_tm1)\n",
    "        _xC = concatenate([x, C])\n",
    "        _xC = Lambda(lambda x: K.reshape(x, (-1, input_dim + hidden_dim)), output_shape=(input_dim + hidden_dim,))(_xC)\n",
    "\n",
    "        alpha = W3(_xC)\n",
    "        alpha = Lambda(lambda x: K.reshape(x, (-1, input_length)), output_shape=(input_length,))(alpha)\n",
    "        alpha = Activation('softmax')(alpha)\n",
    "\n",
    "        _x = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=(1, 1)), output_shape=(input_dim,))([alpha, x])\n",
    "\n",
    "        z = add([W1(_x), U(h_tm1)])\n",
    "\n",
    "        z0, z1, z2, z3 = get_slices(z, 4)\n",
    "\n",
    "        i = Activation(self.recurrent_activation)(z0)\n",
    "        f = Activation(self.recurrent_activation)(z1)\n",
    "\n",
    "        c = add([multiply([f, c_tm1]), multiply([i, Activation(self.activation)(z2)])])\n",
    "        o = Activation(self.recurrent_activation)(z3)\n",
    "        h = multiply([o, Activation(self.activation)(c)])\n",
    "        y = Activation(self.activation)(W2(h))\n",
    "\n",
    "        return Model([x, h_tm1, c_tm1], [y, h, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from recurrentshop import LSTMCell, RecurrentSequential \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, TimeDistributed, Bidirectional, Input, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=5000\n",
    "embedding_size=300\n",
    "enc_input_length = 200\n",
    "training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_385 (InputLayer)          (64, None, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "recurrent_sequential_25 (Recurr (64, None, 128)      482816      input_385[0][0]                  \n",
      "                                                                 private__optional_input_place_hol\n",
      "                                                                 private__optional_input_place_hol\n",
      "                                                                 private__optional_input_place_hol\n",
      "                                                                 private__optional_input_place_hol\n",
      "                                                                 private__optional_input_place_hol\n",
      "                                                                 private__optional_input_place_hol\n",
      "==================================================================================================\n",
      "Total params: 482,816\n",
      "Trainable params: 482,816\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder = RecurrentSequential(unroll=False, stateful=False,\n",
    "                                  return_sequences=True)\n",
    "encoder.add(LSTMCell(128, batch_input_shape=(shape[0], shape[2])))\n",
    "encoder.add(Dropout(0.5))\n",
    "encoder.add(LSTMCell(128))\n",
    "encoder.add(Dropout(0.5))\n",
    "encoder.add(LSTMCell(128))\n",
    "\n",
    "_input = Input(batch_shape=shape)\n",
    "_input._keras_history[0].supports_masking = True\n",
    "encoded = encoder(_input)\n",
    "\n",
    "model = Model(_input, encoded)\n",
    " \n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# 把模型保存为图片\n",
    "from keras.utils import plot_model\n",
    "plot_model(model,to_file='model_png/301.png', show_layer_names=True, show_shapes=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded.shape: (None, 93, 300)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_320 (Model)            (None, 128, 21100)        2942489   \n",
      "=================================================================\n",
      "Total params: 2,942,489\n",
      "Trainable params: 2,942,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "output_dim=21100\n",
    "output_length=128\n",
    "batch_input_shape=None\n",
    "batch_size=None\n",
    "input_shape=None\n",
    "input_length=93\n",
    "input_dim=300\n",
    "hidden_dim=128 \n",
    "depth=1\n",
    "bidirectional=True\n",
    "unroll=False\n",
    "stateful=False\n",
    "dropout=0.0\n",
    "shape=(None, 93, 300)\n",
    "\n",
    "_input = Input(batch_shape=shape)\n",
    "_input._keras_history[0].supports_masking = True\n",
    " \n",
    "\n",
    "print(\"encoded.shape:\", shape)\n",
    "decoder = RecurrentSequential(decode=True, output_length=output_length,\n",
    "                              unroll=unroll, stateful=stateful)\n",
    "decoder.add(Dropout(dropout, batch_input_shape=(shape[0], shape[1], hidden_dim)))\n",
    "decoder.add(AttentionDecoderCell(output_dim=output_dim, hidden_dim=hidden_dim))\n",
    "\n",
    "inputs = [_input]\n",
    "decoded = decoder(_input) \n",
    "model_decoder = Model(inputs, decoded) \n",
    "\n",
    "model = Sequential()\n",
    "model.add(model_decoder)\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1000)) \n",
    "\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim=300\n",
    "output_length=5000\n",
    "batch_input_shape=None\n",
    "batch_size=None\n",
    "input_shape=None\n",
    "input_length=93\n",
    "input_dim=300\n",
    "hidden_dim=128 \n",
    "depth=1\n",
    "bidirectional=True\n",
    "unroll=False\n",
    "stateful=False\n",
    "dropout=0.0\n",
    "\n",
    "shape=(93, None, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded.shape: (None, 93, 300)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_726 (InputLayer)          (None, 93, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "private__optional_input_place_h (2,)                 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "recurrent_sequential_56 (Recurr (None, 5000, 300)    259289      input_726[0][0]                  \n",
      "                                                                 private__optional_input_place_hol\n",
      "                                                                 private__optional_input_place_hol\n",
      "==================================================================================================\n",
      "Total params: 259,289\n",
      "Trainable params: 259,289\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if isinstance(depth, int):\n",
    "    depth = (depth, depth)\n",
    "if batch_input_shape:\n",
    "    shape = batch_input_shape\n",
    "elif input_shape:\n",
    "    shape = (batch_size,) + input_shape\n",
    "elif input_dim:\n",
    "    if input_length:\n",
    "        shape = (batch_size,) + (input_length,) + (input_dim,)\n",
    "    else:\n",
    "        shape = (batch_size,) + (None,) + (input_dim,)\n",
    "else:\n",
    "    # TODO Proper error message\n",
    "    raise TypeError\n",
    "if hidden_dim is None:\n",
    "    hidden_dim = output_dim\n",
    "\n",
    "_input = Input(batch_shape=shape)\n",
    "_input._keras_history[0].supports_masking = True\n",
    "\n",
    "encoder = RecurrentSequential(unroll=unroll, stateful=stateful,\n",
    "                              return_sequences=True)\n",
    "encoder.add(LSTMCell(hidden_dim, batch_input_shape=(shape[0], shape[2])))\n",
    "\n",
    "for _ in range(1, depth[0]):\n",
    "    encoder.add(Dropout(dropout))\n",
    "    encoder.add(LSTMCell(hidden_dim))\n",
    "\n",
    "if bidirectional:\n",
    "    encoder = Bidirectional(encoder, merge_mode='sum')\n",
    "    encoder.forward_layer.build(shape)\n",
    "    encoder.backward_layer.build(shape)\n",
    "    # patch\n",
    "    encoder.layer = encoder.forward_layer\n",
    "\n",
    "encoded = encoder(_input)\n",
    "\n",
    "print(\"encoded.shape:\", shape)\n",
    "decoder = RecurrentSequential(decode=True, output_length=output_length,\n",
    "                              unroll=unroll, stateful=stateful)\n",
    "decoder.add(Dropout(dropout, batch_input_shape=(shape[0], shape[1], hidden_dim)))\n",
    "if depth[1] == 1:\n",
    "    decoder.add(AttentionDecoderCell(output_dim=output_dim, hidden_dim=hidden_dim))\n",
    "else:\n",
    "    decoder.add(AttentionDecoderCell(output_dim=output_dim, hidden_dim=hidden_dim))\n",
    "    for _ in range(depth[1] - 2):\n",
    "        decoder.add(Dropout(dropout))\n",
    "        decoder.add(LSTMDecoderCell(output_dim=hidden_dim, hidden_dim=hidden_dim))\n",
    "    decoder.add(Dropout(dropout))\n",
    "    decoder.add(LSTMDecoderCell(output_dim=output_dim, hidden_dim=hidden_dim))\n",
    "\n",
    "inputs = [_input]\n",
    "decoded = decoder(_input)\n",
    "# decoded = decoder(encoded)\n",
    "model = Model(inputs, decoded)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
