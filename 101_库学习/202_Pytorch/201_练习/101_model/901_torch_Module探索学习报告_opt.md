**ç›®å½•**

- [1 åˆ›å»ºæ¨¡å‹](#1 åˆ›å»ºæ¨¡å‹)
	- [1.1 ç›´æ¥åˆå§‹åŒ– Layer(torché‡Œé¢éƒ½æ˜¯ç»§æ‰¿è‡³Moduleçš„)](#1.1 ç›´æ¥åˆå§‹åŒ– Layer(torché‡Œé¢éƒ½æ˜¯ç»§æ‰¿è‡³Moduleçš„))
	- [1.2 é€šè¿‡ Sequential å¿«é€Ÿæ·»åŠ ](#1.2 é€šè¿‡ Sequential å¿«é€Ÿæ·»åŠ )
	- [1.3 é€šè¿‡ Sequential çš„ add æ–¹æ³•ä¸€ä¸ªä¸ªç»„ä»¶æ·»åŠ ](#1.3 é€šè¿‡ Sequential çš„ add æ–¹æ³•ä¸€ä¸ªä¸ªç»„ä»¶æ·»åŠ )
	- [1.4 é€šè¿‡ Sequential æ·»åŠ  dict æ¥æ·»åŠ ](#1.4 é€šè¿‡ Sequential æ·»åŠ  dict æ¥æ·»åŠ )
- [2 æ¨¡å‹ç¼–è¯‘](#2 æ¨¡å‹ç¼–è¯‘)
	- [2.1 æŸå¤±å‡½æ•°](#2.1 æŸå¤±å‡½æ•°)
	- [2.2 ä¼˜åŒ–å™¨Optimizer](#2.2 ä¼˜åŒ–å™¨Optimizer)
- [3 æ•°æ®å¤„ç†](#3 æ•°æ®å¤„ç†)
	- [3.1 ç›´æ¥ç”¨torchç”Ÿæˆtensor -> Variable](#3.1 ç›´æ¥ç”¨torchç”Ÿæˆtensor -> Variable)
	- [3.2 ä»numpy -> torch tensor -> Variable](#3.2 ä»numpy -> torch tensor -> Variable)
	- [3.3 é‡‡ç”¨è‡ªå¸¦çš„ dataset](#3.3 é‡‡ç”¨è‡ªå¸¦çš„ dataset)
	- [3.4 è‡ªå®šä¹‰çš„æ•°æ®é›†](#3.4 è‡ªå®šä¹‰çš„æ•°æ®é›†)




<font size=5>**torch Module æ¢ç´¢å­¦ä¹ æŠ¥å‘Š**</font>

**æ‘˜è¦ï¼š** åœ¨Githubæœç´¢é¡¹ç›®çš„æ—¶å€™ï¼Œä¼šçœ‹åˆ°å¾ˆå¤štorchçš„èº«å½±ï¼Œtorchçš„æ€§èƒ½åœ¨ä¹‹å‰ä¹Ÿæ˜¯æœ‰æ‰€è€³é—»ï¼Œæ‰€ä»¥æƒ³èƒ½æ›´æ·±å…¥çš„äº†è§£torchè¿™ä¸ªæ¡†æ¶ã€‚å­¦ä¹ çš„è·¯å¾„ä¾ç„¶æ˜¯æŒ‰ç…§å¯¹æ¨¡å‹çš„å­¦ä¹ ã€å¯¹ä¸€äº›å±‚çš„å†…éƒ¨é€»è¾‘çš„äº†è§£(torché‡Œé¢å…¶å®éƒ½æ˜¯Module)ï¼Œç„¶åèƒ½å¯¹Torchæœ‰ä¸€ä¸ªåŸºæœ¬çš„äº†è§£ã€‚é€šè¿‡å¯¹Moduleæ­å»ºæ–¹æ³•ã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°ã€è®­ç»ƒæ–¹æ³•çš„å®è·µï¼ŒåŸºæœ¬äº†è§£äº†torchè¿›è¡Œç¥ç»ç½‘ç»œæ­å»ºå·²ç»è®­ç»ƒçš„æ•´ä¸ªæµç¨‹ã€‚



**æ¢ç´¢å­¦ä¹ è®°å½•**

æœ¬æ¬¡æ¢ç´¢ä¸»è¦ä»ç½‘ç»œæœç´¢ç›¸åº”çš„æ•™ç¨‹ï¼Œç„¶åå»ç†è§£åŒæ—¶é…åˆtorchæºç å»ç†è§£



# <a name="1 åˆ›å»ºæ¨¡å‹">1 åˆ›å»ºæ¨¡å‹</a>

â€‹	torchæ­å»ºæ¨¡å‹ä¸»è¦é€šè¿‡å››ç§æ–¹æ³•ï¼Œæ¯ç§æ–¹æ³•å·®åˆ«ä¸å¤§ï¼Œä½†ä¹Ÿéƒ½æœ‰ç€ä¸€äº›å°çš„åŒºåˆ«ã€‚



## <a name="1.1 ç›´æ¥åˆå§‹åŒ– Layer(torché‡Œé¢éƒ½æ˜¯ç»§æ‰¿è‡³Moduleçš„)">1.1 ç›´æ¥åˆå§‹åŒ– Layer(torché‡Œé¢éƒ½æ˜¯ç»§æ‰¿è‡³Moduleçš„)</a>

```python
import torch
import torch.nn.functional as F
from collections import OrderedDict
from torchsummary import summary

class Net_1(torch.nn.Module):
    def __init__(self):
        super(Net_1, self).__init__()
        # Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        self.conv_1 = torch.nn.Conv2d(3, 32, 3, 1, 1)
        self.pool_1 = torch.nn.MaxPool2d(2)
        self.dense_1 = torch.nn.Linear(32*3*3, 128) # ä¹‹æ‰€ä»¥æ˜¯32*3*3æ˜¯å› ä¸ºä¼šç»è¿‡ä¸€ä¸ªæ± åŒ–å±‚
        self.dense_2 = torch.nn.Linear(128, 10)
    
    def forward(self, x):
        x = F.relu(self.conv_1(x))
        x = self.pool_1(x)
        x = x.view(x.size(0), -1)
        x = F.relu(self.dense_1(x))
        x = self.dense_2(x)
        return x

summary(Net_1(), (3, 6, 6))

"""
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1             [-1, 32, 6, 6]             896
         MaxPool2d-2             [-1, 32, 3, 3]               0
            Linear-3                  [-1, 128]          36,992
            Linear-4                   [-1, 10]           1,290
================================================================
Total params: 39,178
Trainable params: 39,178
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.15
Estimated Total Size (MB): 0.16
----------------------------------------------------------------
"""
```



## <a name="1.2 é€šè¿‡ Sequential å¿«é€Ÿæ·»åŠ ">1.2 é€šè¿‡ Sequential å¿«é€Ÿæ·»åŠ </a>

```python
import torch
import torch.nn.functional as F
from collections import OrderedDict
from torchsummary import summary

# é€šè¿‡ Sequential å¿«é€Ÿæ·»åŠ 
class Net_2(torch.nn.Module):
    def __init__(self):
        super(Net_2, self).__init__()
        self.conv = torch.nn.Sequential(torch.nn.Conv2d(3, 32, 3, 1, 1),
                                        torch.nn.ReLU(), 
                                        torch.nn.MaxPool2d(2))
        self.dense = torch.nn.Sequential(torch.nn.Linear(32*3*3, 128),
                                         torch.nn.ReLU(),
                                         torch.nn.Linear(128, 10))
    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        x = self.dense(x)
        return x
    
summary(Net_2(), (3, 6, 6))   
```



## <a name="1.3 é€šè¿‡ Sequential çš„ add æ–¹æ³•ä¸€ä¸ªä¸ªç»„ä»¶æ·»åŠ ">1.3 é€šè¿‡ Sequential çš„ add æ–¹æ³•ä¸€ä¸ªä¸ªç»„ä»¶æ·»åŠ </a>

```python
import torch
import torch.nn.functional as F
from collections import OrderedDict
from torchsummary import summary

# é€šè¿‡ Sequential çš„ add æ–¹æ³•ä¸€ä¸ªä¸ªç»„ä»¶æ·»åŠ 
class Net_3(torch.nn.Module):
    def __init__(self):
        super(Net_3, self).__init__()
        self.conv = torch.nn.Sequential()
        self.conv.add_module('conv_1', torch.nn.Conv2d(3, 32, 3, 1, 1))
        self.conv.add_module('relu_1', torch.nn.ReLU())
        self.conv.add_module('pool_1', torch.nn.MaxPool2d(2))
        self.dense = torch.nn.Sequential()
        self.dense.add_module('dense_1', torch.nn.Linear(32*3*3, 128))
        self.dense.add_module('relu_2', torch.nn.ReLU())
        self.dense.add_module('dense_2', torch.nn.Linear(128, 10))
    
    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        x = self.dense(x)
        return x
    
summary(Net_3(), (3, 6, 6))  
print(Net_3())
```



## <a name="1.4 é€šè¿‡ Sequential æ·»åŠ  dict æ¥æ·»åŠ ">1.4 é€šè¿‡ Sequential æ·»åŠ  dict æ¥æ·»åŠ </a>

```python
import torch
import torch.nn.functional as F
from collections import OrderedDict
from torchsummary import summary

# é€šè¿‡ Sequential æ·»åŠ  dict æ¥æ·»åŠ 

class Net_4(torch.nn.Module):
    def __init__(self):
        super(Net_4, self).__init__()
        self.conv = torch.nn.Sequential(
            OrderedDict([
                ("conv_1", torch.nn.Conv2d(3, 32, 3, 1, 1)),
                ("rule_1", torch.nn.ReLU()),
                ("pool", torch.nn.MaxPool2d(2))
            ])
        )
        self.dense = torch.nn.Sequential(
            OrderedDict([
                ("dense_1", torch.nn.Linear(32*3*3, 128)),
                ("rule_2", torch.nn.ReLU()),
                ("desnse_2", torch.nn.Linear(128, 10))
            ])
        )
    
    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        x = self.dense(x)
        return x

summary(Net_4(), (3, 6, 6))  
print(Net_4())
```



# <a name="2 æ¨¡å‹ç¼–è¯‘">2 æ¨¡å‹ç¼–è¯‘</a>

æ¨¡å‹ç¼–è¯‘ä¸»è¦æ˜¯è®¾ç½®æŸå¤±å‡½æ•°ã€è®¾ç½®ä¼˜åŒ–å™¨çš„è¿‡ç¨‹(torchåç»­è¿˜éœ€è¦è¿›è¡Œåå‘åå‘ä¼ æ’­ç­‰æ“ä½œ)



**ä¾‹**

```python
import torch 
from torch.autograd import Variable

x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1) # xdata shape = (100,1) 
y = x.pow(2) + 0.2*torch.rand(x.size())
x, y = Variable(x), Variable(y)

class Net(torch.nn.Module):
    def __init__(self, n_feature, n_hidden, n_output):
        super(Net, self).__init__()
        # åŒ…å«çš„å±‚
        self.hidden = torch.nn.Linear(n_feature, n_hidden)
        self.predict = torch.nn.Linear(n_hidden, n_output)
    
    def forward(self, x):
        # å±‚è¿æ¥
        x = torch.relu(self.hidden(x))
        x = self.predict(x)
        return x 

net = Net(1, 10, 1)

optimizer = torch.optim.SGD(net.parameters(), lr=0.5)
loss_func = torch.nn.MSELoss()

for t in range(1000):
    prediction = net(x)
    loss = loss_func(prediction, y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if t%200 == 0:
        print('loss:', loss) 
```



## <a name="2.1 æŸå¤±å‡½æ•°">2.1 æŸå¤±å‡½æ•°</a>

```python
# åŸºæœ¬ç”¨æ³•

criterion = LossCriterion() # æ„å»ºå‡½æ•°è‡ªå·±çš„å‚æ•°
loss = criterion(x, y) # è°ƒç”¨
```

- class torch.nn.L1Loss(size_average=True)
  - åˆ›å»ºä¸€ä¸ªè¡¡é‡è¾“å…¥x(æ¨¡å‹é¢„æµ‹è¾“å‡º)å’Œç›®æ ‡yä¹‹é—´å·®çš„ç»å¯¹å€¼çš„å¹³å‡å€¼çš„æ ‡å‡†ã€‚
- class torch.nn.MSELoss(size_average=True)
  - åˆ›å»ºä¸€ä¸ªè¡¡é‡è¾“å…¥x(æ¨¡å‹é¢„æµ‹è¾“å‡º)å’Œç›®æ ‡yä¹‹é—´å‡æ–¹è¯¯å·®æ ‡å‡†ã€‚
- class torch.nn.CrossEntropyLoss(weight=None, size_average=True)
  - æ­¤æ ‡å‡†å°†LogSoftMaxå’ŒNLLLossé›†æˆåˆ°ä¸€ä¸ªç±»ä¸­
- class torch.nn.NLLLoss(weight=None, size_average=True)
  - è´Ÿçš„log likelihood lossæŸå¤±ã€‚ç”¨äºè®­ç»ƒä¸€ä¸ªnç±»åˆ†ç±»å™¨ã€‚
- class torch.nn.NLLLoss2d(weight=None, size_average=True)
  - å¯¹äºå›¾ç‰‡çš„ negative log likehood lossã€‚è®¡ç®—æ¯ä¸ªåƒç´ çš„ NLL lossã€‚
- class torch.nn.KLDivLoss(weight=None, size_average=True)
  - è®¡ç®— KL æ•£åº¦æŸå¤±
- class torch.nn.BCELoss(weight=None, size_average=True)
  - è®¡ç®— target ä¸ output ä¹‹é—´çš„äºŒè¿›åˆ¶äº¤å‰ç†µ
- class torch.nn.MarginRankingLoss(margin=0, size_average=True)
  - åˆ›å»ºä¸€ä¸ªæ ‡å‡†ï¼Œç»™å®šè¾“å…¥ ğ‘¥1x1,ğ‘¥2x2ä¸¤ä¸ª1-D mini-batch Tensor'sï¼Œå’Œä¸€ä¸ªğ‘¦y(1-D mini-batch tensor) ,ğ‘¦yé‡Œé¢çš„å€¼åªèƒ½æ˜¯-1æˆ–1ã€‚
- class torch.nn.HingeEmbeddingLoss(size_average=True) - 
- class torch.nn.MultiLabelMarginLoss(size_average=True)
- class torch.nn.SmoothL1Loss(size_average=True)
  - å¹³æ»‘ç‰ˆL1 loss
- class torch.nn.SoftMarginLoss(size_average=True)
  - åˆ›å»ºä¸€ä¸ªæ ‡å‡†ï¼Œç”¨æ¥ä¼˜åŒ–2åˆ†ç±»çš„logistic lossã€‚
- class torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=True)
  - åˆ›å»ºä¸€ä¸ªæ ‡å‡†ï¼ŒåŸºäºè¾“å…¥xå’Œç›®æ ‡yçš„ max-entropyï¼Œä¼˜åŒ–å¤šæ ‡ç­¾ one-versus-all çš„æŸå¤±ã€‚
- class torch.nn.CosineEmbeddingLoss(margin=0, size_average=True)
  - ç»™å®š è¾“å…¥ Tensorsï¼Œx1, x2 å’Œä¸€ä¸ªæ ‡ç­¾Tensor y(å…ƒç´ çš„å€¼ä¸º1æˆ–-1)ã€‚
- class torch.nn.MultiMarginLoss(p=1, margin=1, weight=None, size_average=True)
  - ç”¨æ¥è®¡ç®—multi-class classificationçš„hinge lossï¼ˆmagin-based lossï¼‰



## <a name="2.2 ä¼˜åŒ–å™¨Optimizer">2.2 ä¼˜åŒ–å™¨Optimizer</a>



```python
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
optimizer = optim.Adam([var1, var2], lr=0.0001)
```



- class torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)
  - å®ç°Adadeltaç®—æ³•
- class torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0)
  - å®ç°Adagradç®—æ³•
- class torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)
  - å®ç°Adamç®—æ³•
- class torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)
  - å®ç°Adamaxç®—æ³•ï¼ˆAdamçš„ä¸€ç§åŸºäºæ— ç©·èŒƒæ•°çš„å˜ç§ï¼‰
- class torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)
  - å®ç°å¹³å‡éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ã€‚
- class torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-05, tolerance_change=1e-09, history_size=100, line_search_fn=None)
  - å®ç°L-BFGSç®—æ³•
- class torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)
  - å®ç°RMSpropç®—æ³•
- class torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50))
  - å®ç°å¼¹æ€§åå‘ä¼ æ’­ç®—æ³•ã€‚
- class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)
  - å®ç°éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼ˆmomentumå¯é€‰ï¼‰



# <a name="3 æ•°æ®å¤„ç†">3 æ•°æ®å¤„ç†</a>

æ•°æ®å¤„ç†å¯ä»¥ç”¨å¤šå°‘ç§æ–¹å¼ï¼Œä½†æ˜¯æœ€ç»ˆéƒ½éœ€è¦å¤„ç†æˆtorchéœ€è¦çš„æ•°æ®ç±»å‹ï¼Œå¦åˆ™ä¼šæŠ¥é”™



ä¸»è¦æ¢ç´¢çš„å¤„ç†æ–¹å¼:

- ç›´æ¥ç”¨torchç”Ÿæˆtensor -> Variable
- ä»numpy -> torch tensor -> Variable
- é‡‡ç”¨è‡ªå¸¦çš„ dataset
- è‡ªå®šä¹‰çš„æ•°æ®é›†



## <a name="3.1 ç›´æ¥ç”¨torchç”Ÿæˆtensor -> Variable">3.1 ç›´æ¥ç”¨torchç”Ÿæˆtensor -> Variable</a>

```python
import torch 
from torch.autograd import Variable

x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1) # xdata shape = (100,1) 
y = x.pow(2) + 0.2*torch.rand(x.size())
x, y = Variable(x), Variable(y)

class Net(torch.nn.Module):
    def __init__(self, n_feature, n_hidden, n_output):
        super(Net, self).__init__()
        # åŒ…å«çš„å±‚
        self.hidden = torch.nn.Linear(n_feature, n_hidden)
        self.predict = torch.nn.Linear(n_hidden, n_output)
    
    def forward(self, x):
        # å±‚è¿æ¥
        x = torch.relu(self.hidden(x))
        x = self.predict(x)
        return x 

net = Net(1, 10, 1)

optimizer = torch.optim.SGD(net.parameters(), lr=0.5)
loss_func = torch.nn.MSELoss()

for t in range(1000):
    prediction = net(x)
    loss = loss_func(prediction, y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if t%200 == 0:
        print('loss:', loss) 
```



## <a name="3.2 ä»numpy -> torch tensor -> Variable">3.2 ä»numpy -> torch tensor -> Variable</a>

```python
import torch 
from torch.autograd import Variable
import numpy as np

x = np.linspace(-1, 1, 100).reshape((100, -1)).astype(np.float32)
y = x**2 + 0.2*np.random.random(x.shape).astype(np.float32)  

x, y = Variable(torch.from_numpy(x)), Variable(torch.from_numpy(y)) 

class Net(torch.nn.Module):
    def __init__(self, n_feature, n_hidden, n_output):
        super(Net, self).__init__()
        # åŒ…å«çš„å±‚
        self.hidden = torch.nn.Linear(n_feature, n_hidden)
        self.predict = torch.nn.Linear(n_hidden, n_output)
    
    def forward(self, x):
        # å±‚è¿æ¥
        x = torch.relu(self.hidden(x))
        x = self.predict(x)
        return x 

net = Net(1, 10, 1)

optimizer = torch.optim.SGD(net.parameters(), lr=0.5)
loss_func = torch.nn.MSELoss()

for t in range(1000):
    prediction = net(x)
    loss = loss_func(prediction, y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if t%200 == 0:
        print('loss:', loss) 
```



## <a name="3.3 é‡‡ç”¨è‡ªå¸¦çš„ dataset">3.3 é‡‡ç”¨è‡ªå¸¦çš„ dataset</a>

```python
import torch
from torch import nn
from torch.autograd import Variable
import torchvision.datasets as dsets

import torchvision.transforms as transforms
import matplotlib.pyplot as plt
%matplotlib inline

torch.manual_seed(1)

# Hyper Parameters
EPOCH = 1               # train the training data n times, to save time, we just train 1 epoch
BATCH_SIZE = 64
TIME_STEP = 28          # rnn time step / image height
INPUT_SIZE = 28         # rnn input size / image width
LR = 0.01               # learning rate
DOWNLOAD_MNIST = True   # set to True if haven't download the data

mnist_base_path="/Users/zhouwencheng/Desktop/Grass/data/picture/mnist"
train_data = dsets.MNIST(
    root = mnist_base_path,
    train=True,
    transform=transforms.ToTensor(), # (0, 1)
    download=DOWNLOAD_MNIST
)

print(train_data.train_data.size())
print(train_data.targets.size())

train_loader = torch.utils.data.DataLoader(dataset = train_data,
                                           batch_size=BATCH_SIZE,
                                           shuffle=True)
test_data = dsets.MNIST(root=mnist_base_path, 
                        train=False, 
                        transform=transforms.ToTensor())
test_x = Variable(test_data.test_data, volatile=True).type(torch.FloatTensor)[:2000]/255.
test_y = test_data.targets.numpy().squeeze()[:2000]

class RNN(nn.Module):
    def __init__(self):
        super(RNN, self).__init__()
        self.rnn = nn.LSTM(               # if use nn.RNN(), it hardly learns
            input_size = INPUT_SIZE,
            hidden_size = 64,     # rnn hidden unit
            num_layers = 1,        # number of rnn layer
            batch_first = True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)
        )
        self.out = nn.Linear(64, 10)
        
    def forward(self, x):
        # x shape (batch, time_step, input_size)
        # r_out shape (batch, time_step, output_size)
        # h_n shape (n_layers, batch, hidden_size)
        # h_c shape (n_layers, batch, hidden_size)
        r_out, (h_n, h_c) = self.rnn(x, None) # None represents zero initial hidden state

        # choose r_out at the last time step
        out = self.out(r_out[:, -1,:])
        return out
    
rnn = RNN()
print(rnn)

optimizer = torch.optim.Adam(rnn.parameters(), lr=LR) # optimize all cnn parameters
loss_func = nn.CrossEntropyLoss() # the target label is not one-hotted

# train and testing
for epoch in range(EPOCH):
    for step, (x, y) in enumerate(train_loader):
        b_x = Variable(x.view(-1, 28, 28))
        b_y = Variable(y)
        
        output = rnn(b_x)
        loss = loss_func(output, b_y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if step % 200 == 0:
            test_output = rnn(test_x)
            pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()
            accuracy = sum(pred_y == test_y)/float(test_y.size)
            print(f'Epoch:  {epoch} | step:{step} | train loss: {loss.data} | test accuracy: {accuracy}')
#             print(loss.data)
    print("OK")
            
```



## <a name="3.4 è‡ªå®šä¹‰çš„æ•°æ®é›†">3.4 è‡ªå®šä¹‰çš„æ•°æ®é›†</a>

```python
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torch.autograd import Variable
import torchvision.datasets as dsets
import torchvision.transforms as transforms 
import numpy as np


class CustomDataset(Dataset):#éœ€è¦ç»§æ‰¿data.Dataset
    
    def __init__(self):
        # TODO
        # 1. Initialize file path or list of file names.
        pass
    
    def __getitem__(self, index):
        # TODO
        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).
        # 2. Preprocess the data (e.g. torchvision.Transform).
        # 3. Return a data pair (e.g. image and label).
        #è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç¬¬ä¸€æ­¥ï¼šread one dataï¼Œæ˜¯ä¸€ä¸ªdata
        
        data = np.random.random(10).astype(np.float32)
        label = np.random.randint(0, 2)  

        data = Variable(torch.from_numpy(data)) 
        return data, label 
    
    def __len__(self):
        # You should change 0 to the total size of your dataset.
        return 1000
    
a = 100
train_data = CustomDataset()
train_loader = DataLoader(dataset=train_data, batch_size=20, shuffle=False)


class Net(torch.nn.Module):
    def __init__(self, n_feature, n_hidden, n_output):
        super(Net, self).__init__()
        # åŒ…å«çš„å±‚
        self.hidden = torch.nn.Linear(n_feature, n_hidden)
        self.predict = torch.nn.Linear(n_hidden, n_output)
    
    def forward(self, x):
        # å±‚è¿æ¥
        x = torch.relu(self.hidden(x))
        x = self.predict(x)
        return x 

net = Net(10, 10, 2)

print(net)

optimizer = torch.optim.Adam(rnn.parameters(), lr=LR) # optimize all cnn parameters
loss_func = nn.CrossEntropyLoss() # the target label is not one-hotted

# train and testing
for epoch in range(EPOCH):
    for step, (x, y) in enumerate(train_loader):
        x = Variable(x)
        y = Variable(y)
        output = net(x) 
        loss = loss_func(output, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step() 
        if step%10 == 0 :
            print(loss)
    


```

