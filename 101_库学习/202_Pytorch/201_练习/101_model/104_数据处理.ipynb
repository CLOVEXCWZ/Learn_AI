{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理方式：\n",
    "- 直接用torch生成tensor -> Variable\n",
    "- 从numpy -> torch tensor -> Variable\n",
    "- 采用自带的 dataset\n",
    "- 自定义的数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 直接用torch生成tensor -> Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.4932, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.0168, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.0040, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1) # xdata shape = (100,1) \n",
    "y = x.pow(2) + 0.2*torch.rand(x.size())\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        # 包含的层\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 层连接\n",
    "        x = torch.relu(self.hidden(x))\n",
    "        x = self.predict(x)\n",
    "        return x \n",
    "\n",
    "net = Net(1, 10, 1)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.5)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "for t in range(1000):\n",
    "    prediction = net(x)\n",
    "    loss = loss_func(prediction, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if t%200 == 0:\n",
    "        print('loss:', loss) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从numpy -> torch tensor -> Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.4710, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.0611, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.0610, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.0610, grad_fn=<MseLossBackward>)\n",
      "loss: tensor(0.0610, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(-1, 1, 100).reshape((100, -1)).astype(np.float32)\n",
    "y = x**2 + 0.2*np.random.random(x.shape).astype(np.float32)  \n",
    "\n",
    "x, y = Variable(torch.from_numpy(x)), Variable(torch.from_numpy(y)) \n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        # 包含的层\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 层连接\n",
    "        x = torch.relu(self.hidden(x))\n",
    "        x = self.predict(x)\n",
    "        return x \n",
    "\n",
    "net = Net(1, 10, 1)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.5)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "for t in range(1000):\n",
    "    prediction = net(x)\n",
    "    loss = loss_func(prediction, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if t%200 == 0:\n",
    "        print('loss:', loss) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 采用自带的 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n",
      "RNN(\n",
      "  (rnn): LSTM(28, 64, batch_first=True)\n",
      "  (out): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhouwencheng/Desktop/Grass/101PythonEnv/envpy3.7/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | step:0 | train loss: 2.2883260250091553 | test accuracy: 0.1025\n",
      "Epoch:  0 | step:200 | train loss: 0.4038448631763458 | test accuracy: 0.8375\n",
      "Epoch:  0 | step:400 | train loss: 0.21243950724601746 | test accuracy: 0.913\n",
      "Epoch:  0 | step:600 | train loss: 0.05201313644647598 | test accuracy: 0.9405\n",
      "Epoch:  0 | step:800 | train loss: 0.12530261278152466 | test accuracy: 0.942\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Hyper Parameters\n",
    "EPOCH = 1               # train the training data n times, to save time, we just train 1 epoch\n",
    "BATCH_SIZE = 64\n",
    "TIME_STEP = 28          # rnn time step / image height\n",
    "INPUT_SIZE = 28         # rnn input size / image width\n",
    "LR = 0.01               # learning rate\n",
    "DOWNLOAD_MNIST = True   # set to True if haven't download the data\n",
    "\n",
    "mnist_base_path=\"/Users/zhouwencheng/Desktop/Grass/data/picture/mnist\"\n",
    "train_data = dsets.MNIST(\n",
    "    root = mnist_base_path,\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(), # (0, 1)\n",
    "    download=DOWNLOAD_MNIST\n",
    ")\n",
    "\n",
    "print(train_data.train_data.size())\n",
    "print(train_data.targets.size())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_data,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True)\n",
    "test_data = dsets.MNIST(root=mnist_base_path, \n",
    "                        train=False, \n",
    "                        transform=transforms.ToTensor())\n",
    "test_x = Variable(test_data.test_data, volatile=True).type(torch.FloatTensor)[:2000]/255.\n",
    "test_y = test_data.targets.numpy().squeeze()[:2000]\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(               # if use nn.RNN(), it hardly learns\n",
    "            input_size = INPUT_SIZE,\n",
    "            hidden_size = 64,     # rnn hidden unit\n",
    "            num_layers = 1,        # number of rnn layer\n",
    "            batch_first = True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None) # None represents zero initial hidden state\n",
    "\n",
    "        # choose r_out at the last time step\n",
    "        out = self.out(r_out[:, -1,:])\n",
    "        return out\n",
    "    \n",
    "rnn = RNN()\n",
    "print(rnn)\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR) # optimize all cnn parameters\n",
    "loss_func = nn.CrossEntropyLoss() # the target label is not one-hotted\n",
    "\n",
    "# train and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        b_x = Variable(x.view(-1, 28, 28))\n",
    "        b_y = Variable(y)\n",
    "        \n",
    "        output = rnn(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            test_output = rnn(test_x)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "            accuracy = sum(pred_y == test_y)/float(test_y.size)\n",
    "            print(f'Epoch:  {epoch} | step:{step} | train loss: {loss.data} | test accuracy: {accuracy}')\n",
    "#             print(loss.data)\n",
    "    print(\"OK\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (predict): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n",
      "tensor(0.6767, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6240, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6816, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7057, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6983, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):#需要继承data.Dataset\n",
    "    \n",
    "    def __init__(self):\n",
    "        # TODO\n",
    "        # 1. Initialize file path or list of file names.\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # TODO\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        #这里需要注意的是，第一步：read one data，是一个data\n",
    "        \n",
    "        data = np.random.random(10).astype(np.float32)\n",
    "        label = np.random.randint(0, 2)  \n",
    "\n",
    "        data = Variable(torch.from_numpy(data)) \n",
    "        return data, label \n",
    "    \n",
    "    def __len__(self):\n",
    "        # You should change 0 to the total size of your dataset.\n",
    "        return 1000\n",
    "    \n",
    "a = 100\n",
    "train_data = CustomDataset()\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=20, shuffle=False)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        # 包含的层\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 层连接\n",
    "        x = torch.relu(self.hidden(x))\n",
    "        x = self.predict(x)\n",
    "        return x \n",
    "\n",
    "net = Net(10, 10, 2)\n",
    "\n",
    "print(net)\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR) # optimize all cnn parameters\n",
    "loss_func = nn.CrossEntropyLoss() # the target label is not one-hotted\n",
    "\n",
    "# train and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        x = Variable(x)\n",
    "        y = Variable(y)\n",
    "        output = net(x) \n",
    "        loss = loss_func(output, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        if step%10 == 0 :\n",
    "            print(loss)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random(10)\n",
    "np.random.randint(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
