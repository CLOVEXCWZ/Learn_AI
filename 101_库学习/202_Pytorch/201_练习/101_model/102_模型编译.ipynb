{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编译模型\n",
    "\n",
    "- 损失函数\n",
    "- 优化器\n",
    "- 评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n",
      "RNN(\n",
      "  (rnn): LSTM(28, 64, batch_first=True)\n",
      "  (out): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhouwencheng/Desktop/Grass/101PythonEnv/envpy3.7/lib/python3.7/site-packages/ipykernel_launcher.py:36: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | step:0 | train loss: 2.2883260250091553 | test accuracy: 0.1025\n",
      "Epoch:  0 | step:100 | train loss: 0.9628912210464478 | test accuracy: 0.733\n",
      "Epoch:  0 | step:200 | train loss: 0.4038448631763458 | test accuracy: 0.8375\n",
      "Epoch:  0 | step:300 | train loss: 0.2836182117462158 | test accuracy: 0.8975\n",
      "Epoch:  0 | step:400 | train loss: 0.21243950724601746 | test accuracy: 0.913\n",
      "Epoch:  0 | step:500 | train loss: 0.11269920319318771 | test accuracy: 0.9175\n",
      "Epoch:  0 | step:600 | train loss: 0.05201313644647598 | test accuracy: 0.9405\n",
      "Epoch:  0 | step:700 | train loss: 0.13574619591236115 | test accuracy: 0.955\n",
      "Epoch:  0 | step:800 | train loss: 0.12530261278152466 | test accuracy: 0.942\n",
      "Epoch:  0 | step:900 | train loss: 0.18430180847644806 | test accuracy: 0.946\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Hyper Parameters\n",
    "EPOCH = 1               # train the training data n times, to save time, we just train 1 epoch\n",
    "BATCH_SIZE = 64\n",
    "TIME_STEP = 28          # rnn time step / image height\n",
    "INPUT_SIZE = 28         # rnn input size / image width\n",
    "LR = 0.01               # learning rate\n",
    "DOWNLOAD_MNIST = True   # set to True if haven't download the data\n",
    "\n",
    "mnist_base_path=\"/Users/zhouwencheng/Desktop/Grass/data/picture/mnist\"\n",
    "train_data = dsets.MNIST(\n",
    "    root = mnist_base_path,\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(), # (0, 1)\n",
    "    download=DOWNLOAD_MNIST\n",
    ")\n",
    "\n",
    "print(train_data.train_data.size())\n",
    "print(train_data.targets.size())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_data,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True)\n",
    "test_data = dsets.MNIST(root=mnist_base_path, \n",
    "                        train=False, \n",
    "                        transform=transforms.ToTensor())\n",
    "test_x = Variable(test_data.test_data, volatile=True).type(torch.FloatTensor)[:2000]/255.\n",
    "test_y = test_data.targets.numpy().squeeze()[:2000]\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(               # if use nn.RNN(), it hardly learns\n",
    "            input_size = INPUT_SIZE,\n",
    "            hidden_size = 64,     # rnn hidden unit\n",
    "            num_layers = 1,        # number of rnn layer\n",
    "            batch_first = True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None) # None represents zero initial hidden state\n",
    "\n",
    "        # choose r_out at the last time step\n",
    "        out = self.out(r_out[:, -1,:])\n",
    "        return out\n",
    "    \n",
    "rnn = RNN()\n",
    "print(rnn)\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR) # optimize all cnn parameters\n",
    "loss_func = nn.CrossEntropyLoss() # the target label is not one-hotted\n",
    "\n",
    "# train and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        b_x = Variable(x.view(-1, 28, 28))\n",
    "        b_y = Variable(y)\n",
    "        \n",
    "        output = rnn(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            test_output = rnn(test_x)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "            accuracy = sum(pred_y == test_y)/float(test_y.size)\n",
    "            print(f'Epoch:  {epoch} | step:{step} | train loss: {loss.data} | test accuracy: {accuracy}')\n",
    "#             print(loss.data)\n",
    "    print(\"OK\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss funcations\n",
    "\n",
    "\n",
    "[参考地址=======torch中文文档](https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/#loss-functions)\n",
    "\n",
    "基本用法\n",
    "```python\n",
    "criterion = LossCriterion() # 构建函数自己的参数\n",
    "loss = criterion(x, y) # 调用\n",
    "```\n",
    "\n",
    "- class torch.nn.L1Loss(size_average=True)\n",
    "    - 创建一个衡量输入x(模型预测输出)和目标y之间差的绝对值的平均值的标准。\n",
    "    \n",
    "- class torch.nn.MSELoss(size_average=True)\n",
    "    - 创建一个衡量输入x(模型预测输出)和目标y之间均方误差标准。\n",
    "- class torch.nn.CrossEntropyLoss(weight=None, size_average=True)\n",
    "    - 此标准将LogSoftMax和NLLLoss集成到一个类中\n",
    "- class torch.nn.NLLLoss(weight=None, size_average=True)\n",
    "    - 负的log likelihood loss损失。用于训练一个n类分类器。\n",
    "- class torch.nn.NLLLoss2d(weight=None, size_average=True)\n",
    "    - 对于图片的 negative log likehood loss。计算每个像素的 NLL loss。\n",
    "- class torch.nn.KLDivLoss(weight=None, size_average=True)\n",
    "    - 计算 KL 散度损失\n",
    "- class torch.nn.BCELoss(weight=None, size_average=True)\n",
    "    - 计算 target 与 output 之间的二进制交叉熵\n",
    "- class torch.nn.MarginRankingLoss(margin=0, size_average=True)\n",
    "    - 创建一个标准，给定输入 $x1$,$x2$两个1-D mini-batch Tensor's，和一个$y$(1-D mini-batch tensor) ,$y$里面的值只能是-1或1。\n",
    "- class torch.nn.HingeEmbeddingLoss(size_average=True)\n",
    "    - \n",
    "- class torch.nn.MultiLabelMarginLoss(size_average=True)\n",
    "- class torch.nn.SmoothL1Loss(size_average=True)\n",
    "    - 平滑版L1 loss\n",
    "- class torch.nn.SoftMarginLoss(size_average=True)\n",
    "    - 创建一个标准，用来优化2分类的logistic loss。\n",
    "- class torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=True)\n",
    "    - 创建一个标准，基于输入x和目标y的 max-entropy，优化多标签 one-versus-all 的损失。\n",
    "- class torch.nn.CosineEmbeddingLoss(margin=0, size_average=True)\n",
    "    - 给定 输入 Tensors，x1, x2 和一个标签Tensor y(元素的值为1或-1)。\n",
    "- class torch.nn.MultiMarginLoss(p=1, margin=1, weight=None, size_average=True)\n",
    "    - 用来计算multi-class classification的hinge loss（magin-based loss）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "torch.optim\n",
    "\n",
    "[参考地址---------- torch中文文档](https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-optim/#torchoptim)\n",
    "\n",
    "```python\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.Adam([var1, var2], lr=0.0001)\n",
    "```\n",
    "\n",
    "- class torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "    - 实现Adadelta算法\n",
    "\n",
    "- class torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0)\n",
    "    - 实现Adagrad算法\n",
    "\n",
    "- class torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    - 实现Adam算法\n",
    "    \n",
    "- class torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    - 实现Adamax算法（Adam的一种基于无穷范数的变种）\n",
    "- class torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)\n",
    "    - 实现平均随机梯度下降算法。\n",
    "- class torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-05, tolerance_change=1e-09, history_size=100, line_search_fn=None)\n",
    "    - 实现L-BFGS算法\n",
    "- class torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "    - 实现RMSprop算法\n",
    "- class torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50))\n",
    "    - 实现弹性反向传播算法。\n",
    "- class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "    - 实现随机梯度下降算法（momentum可选）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评价指标\n",
    "\n",
    "- 官方没有实现，需要自己计算咯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
