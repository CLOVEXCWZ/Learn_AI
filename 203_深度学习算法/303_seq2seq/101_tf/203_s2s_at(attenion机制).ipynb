{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "class Seq2Seq(object):\n",
    "    def __init__(self,\n",
    "                 source_vocab_size,  # Int 源数据 covab大小\n",
    "                 target_vocab_size,  # Int 目标数据 vocab大小\n",
    "                 target_start_flag_index=0,  # Int 目标数据开始标记\n",
    "                 target_end_flag_index=1,  # Int 目标数据介绍标记\n",
    "                 batch_size=32,  # Int batch大小\n",
    "                 encode_embed_dim=128,  # Int encode_dim 大小\n",
    "                 decode_embed_dim=128,  # Int decoder_dim 大小\n",
    "                 max_pred_len=128,  # Int 预测时最大长度(预测时需要)\n",
    "                 rnn_size=128,  # Int 一层rnn的神经元格式\n",
    "                 num_layers=2,  # Int 层数\n",
    "                 learning_rate=0.001,  # float  学习率\n",
    "                 train_mode=True,  # bool 是否为训练模式\n",
    "                 ):\n",
    "\n",
    "        self.source_vocab_size = source_vocab_size\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.target_start_flag_index = target_start_flag_index\n",
    "        self.target_end_flag_index = target_end_flag_index\n",
    "        self.batch_size = batch_size\n",
    "        self.encode_embed_dim = encode_embed_dim\n",
    "        self.decode_embed_dim = decode_embed_dim\n",
    "        self.rnn_size = rnn_size\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_mode = train_mode\n",
    "        self.max_pred_len = max_pred_len\n",
    "\n",
    "        self.build_model()  # 创建模型\n",
    "\n",
    "    def get_inputs(self):\n",
    "        \"\"\" 创建 placeholder \"\"\"\n",
    "        self.inputs = tf.placeholder(tf.int32, (None, None), name='inputs')  # 输入原句 (None, None)\n",
    "        self.source_sequence_length = tf.placeholder(tf.int32, (None,), name='source_sequence_length')  # 原数据长度-(None,)\n",
    "        if self.train_mode:\n",
    "            self.targets = tf.placeholder(tf.int32, (None, None), name='targets')  # 目标句子 (None, None)\n",
    "            self.target_sequence_length = tf.placeholder(tf.int32, (None,), name='target_sequence_length')  # 目标数据长度 (None, )\n",
    "            self.max_target_sequence_length = tf.reduce_max(self.target_sequence_length, name='max_target_len')  # 最大目标长度\n",
    "\n",
    "    def get_encoder_layer(self,\n",
    "                          input_data,  # 输入tensor （None, None）\n",
    "                          source_sequence_length):  # 源数据的序列长度\n",
    "        \"\"\"\n",
    "        构建encoder层\n",
    "        :param input_data: (None, None)\n",
    "        :param source_sequence_length: (None,)\n",
    "        :return: encoder_output  encoder_state\n",
    "        \"\"\"\n",
    "\n",
    "        # (?, ?, 128) (batch_size, None, dim)\n",
    "        encoder_embed_input = tf.contrib.layers.embed_sequence(ids=input_data,\n",
    "                                                               vocab_size=self.source_vocab_size,\n",
    "                                                               embed_dim=self.encode_embed_dim)\n",
    "        def get_lstm_cell(rnn_size):\n",
    "            return tf.contrib.rnn.LSTMCell(num_units=rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(self.rnn_size) for _ in range(self.num_layers)])\n",
    "        encoder_output, encoder_state = tf.nn.dynamic_rnn(cell=cell,\n",
    "                                                          inputs=encoder_embed_input,\n",
    "                                                          sequence_length=source_sequence_length,\n",
    "                                                          dtype=tf.float32)\n",
    "\n",
    "        # encoder_output (?, ?, 128) (batch_size, None, rnn_size)\n",
    "        # encoder_state Tuple((None, 128), (None, 128))\n",
    "        return encoder_output, encoder_state\n",
    "\n",
    "    def process_decoder_input(self, data):\n",
    "        \"\"\"\n",
    "        把最后一个字符移除，前面添加一个 start_flag_index\n",
    "        例如：  A B C D <EOS>       (<EOS> 为结束标识符)\n",
    "        --> <GO> A B C D           （<GO> 为开始标识符）\n",
    "        \"\"\"\n",
    "        ''' 补充start_flag，并移除最后一个字符 '''\n",
    "        ending = tf.strided_slice(data, [0, 0], [self.batch_size, -1], [1, 1])  # cut掉最后一个字符\n",
    "        decoder_input = tf.concat([tf.fill([self.batch_size, 1], self.target_start_flag_index), ending], 1)\n",
    "        return decoder_input\n",
    "\n",
    "    def decoding_layer(self,\n",
    "                       source_sequence_length,  # 源数据长度\n",
    "                       encoder_outputs,  # 添加一个注意力机制\n",
    "                       encoder_state,    # encode 的状态\n",
    "                       decoder_input=None,  # decoder端输入\n",
    "                       target_sequence_length=None,  # target数据序列长度\n",
    "                       max_target_sequence_length=None, ):  # target数据序列最大长度\n",
    "\n",
    "        decoder_embeddings = tf.Variable(tf.random_uniform([self.target_vocab_size, self.decode_embed_dim]))\n",
    "\n",
    "        def get_decoder_cell(rnn_size):\n",
    "            decoder_cell = tf.contrib.rnn.LSTMCell(num_units=rnn_size,\n",
    "                                                   initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            return decoder_cell\n",
    "\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([get_decoder_cell(self.rnn_size) for _ in range(self.num_layers)])\n",
    "\n",
    "        # attention层\n",
    "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_units=self.rnn_size,\n",
    "                                                                memory=encoder_outputs,\n",
    "                                                                memory_sequence_length=source_sequence_length)\n",
    "        cell = tf.contrib.seq2seq.AttentionWrapper(cell=cell,\n",
    "                                                   attention_mechanism=attention_mechanism,\n",
    "                                                   attention_layer_size=self.rnn_size)\n",
    "        decoder_initial_state = cell.zero_state(batch_size=self.batch_size,\n",
    "                                                dtype=tf.float32).clone(cell_state=encoder_state)\n",
    "\n",
    "        #  Output全连接层\n",
    "        output_layer = Dense(units=self.target_vocab_size,\n",
    "                             kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "        # 4. Training decoder\n",
    "        with tf.variable_scope(\"decode\"):\n",
    "            if self.train_mode:\n",
    "                #  Embedding\n",
    "                decoder_embed_input = tf.nn.embedding_lookup(params=decoder_embeddings,\n",
    "                                                             ids=decoder_input)\n",
    "                # 得到help对象\n",
    "                training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embed_input,\n",
    "                                                                    sequence_length=target_sequence_length,\n",
    "                                                                    time_major=False)\n",
    "                # 构造decoder decoder_initial_state\n",
    "                training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=cell,\n",
    "                                                                   helper=training_helper,\n",
    "                                                                   initial_state=decoder_initial_state,\n",
    "                                                                   output_layer=output_layer)\n",
    "                training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=training_decoder,\n",
    "                                                                                  impute_finished=True,\n",
    "                                                                                  maximum_iterations=max_target_sequence_length)\n",
    "                self.training_decoder_output = training_decoder_output\n",
    "            else:\n",
    "                # 创建一个常量tensor并复制为batch_size的大小\n",
    "                start_tokens = tf.tile(tf.constant([self.target_start_flag_index], dtype=tf.int32), [self.batch_size],\n",
    "                                       name='start_tokens')\n",
    "                predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding=decoder_embeddings,\n",
    "                                                                             start_tokens=start_tokens,\n",
    "                                                                             end_token=self.target_end_flag_index)\n",
    "\n",
    "                # decoder_initial_state\n",
    "                predicting_decoder = tf.contrib.seq2seq.BasicDecoder(cell,\n",
    "                                                                     predicting_helper,\n",
    "                                                                     initial_state=decoder_initial_state\n",
    "                                                                     , output_layer=output_layer)\n",
    "                predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=predicting_decoder,\n",
    "                                                                                    impute_finished=True,\n",
    "                                                                                    maximum_iterations=self.max_pred_len)\n",
    "                self.predicting_decoder_output = predicting_decoder_output\n",
    "                self.predicting_decoder_output = self.predicting_decoder_output.sample_id\n",
    "\n",
    "    def build_model(self):\n",
    "        self.get_inputs()\n",
    "        encoder_outputs, encoder_state = self.get_encoder_layer(input_data=self.inputs,\n",
    "                                                                source_sequence_length=self.source_sequence_length)\n",
    "\n",
    "        if self.train_mode:\n",
    "            decoder_input = self.process_decoder_input(self.targets)  # 预处理后的decoder输入\n",
    "            self.decoding_layer(\n",
    "                source_sequence_length=self.source_sequence_length,\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                encoder_state=encoder_state,\n",
    "                target_sequence_length=self.target_sequence_length,\n",
    "                max_target_sequence_length=self.max_target_sequence_length,\n",
    "                decoder_input=decoder_input, )\n",
    "            self.masks = tf.sequence_mask(lengths=self.target_sequence_length,\n",
    "                                          maxlen=self.max_target_sequence_length,\n",
    "                                          dtype=tf.float32, name='masks')\n",
    "            self.loss = tf.contrib.seq2seq.sequence_loss(logits=self.training_decoder_output.rnn_output,\n",
    "                                                         targets=self.targets,\n",
    "                                                         weights=self.masks,\n",
    "                                                         )\n",
    "            self.opt = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            gradients = self.opt.compute_gradients(self.loss)\n",
    "            capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "            self.update = self.opt.apply_gradients(capped_gradients)\n",
    "        else:\n",
    "            self.decoding_layer(\n",
    "                source_sequence_length=self.source_sequence_length,\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                encoder_state=encoder_state)\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def train(self,\n",
    "              sess,\n",
    "              encoder_inputs,\n",
    "              encoder_inputs_length,\n",
    "              decoder_inputs,\n",
    "              decoder_inputs_length):\n",
    "        input_feed = {\n",
    "            self.inputs.name: encoder_inputs,\n",
    "            self.source_sequence_length.name: encoder_inputs_length,\n",
    "            self.targets.name: decoder_inputs,\n",
    "            self.target_sequence_length.name: decoder_inputs_length\n",
    "        }\n",
    "\n",
    "        output_feed = [\n",
    "            self.update,\n",
    "            self.loss]\n",
    "        _, loss = sess.run(output_feed, input_feed)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, sess, encoder_inputs, encoder_inputs_length):\n",
    "        \"\"\"\n",
    "        预测\n",
    "        :param sess: tensorflow Session\n",
    "        :param encoder_inputs: (batch_size, None) 二维数组\n",
    "        :param encoder_inputs_length: (None,)  一维数组\n",
    "\n",
    "        :return: (batch_size, max_pred_len) 二维数组\n",
    "        \"\"\"\n",
    "        input_feed = {\n",
    "            self.inputs.name: encoder_inputs,\n",
    "            self.source_sequence_length.name: encoder_inputs_length\n",
    "        }\n",
    "        pred = sess.run(self.predicting_decoder_output, input_feed)\n",
    "        return pred\n",
    "\n",
    "    def save(self, sess, save_path):\n",
    "        \"\"\"\n",
    "        保存模型\n",
    "        :param sess: tensorflow Session\n",
    "        :param save_path: 保存地址\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.saver.save(sess, save_path=save_path)\n",
    "\n",
    "    def load(self, sess, save_path):\n",
    "        \"\"\"\n",
    "        加载模型\n",
    "        :param sess: tensorflow Session\n",
    "        :param save_path: 加载地址\n",
    "        :return:None\n",
    "        \"\"\"\n",
    "        self.saver.restore(sess, save_path)\n",
    "\n",
    "print(\"OK\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#  数据处理\n",
    "\n",
    "source_path=\"/Users/zhouwencheng/Desktop/Grass/data/txt/letters/letters_source.txt\"\n",
    "target_path=\"/Users/zhouwencheng/Desktop/Grass/data/txt/letters/letters_up.txt\"\n",
    "\n",
    "# 打开文件\n",
    "with open(source_path, 'r') as f:\n",
    "    source_text=f.read()\n",
    "    source_texts=source_text.split('\\n') # 进行分句后的句子\n",
    "with open(target_path, 'r') as f:\n",
    "    target_text=f.read()\n",
    "    target_texts=target_text.split('\\n')\n",
    "\n",
    "#====== 特殊标记定义 =========#\n",
    "start_flag='<GO>'\n",
    "end_flag='<EOS>'\n",
    "pad_flag='<PAD>'\n",
    "unk_flag='<UNK>'\n",
    "\n",
    "start_index=0\n",
    "end_index=1\n",
    "pad_index=2\n",
    "unk_index=3\n",
    "    \n",
    "all_text=source_text.replace(\"\\n\", \"\")+target_text.replace(\"\\n\", \"\") # 获取全部文本数据\n",
    "char_list=sorted(list(set(all_text))) # 获取字符的数组\n",
    "char_list=[start_flag, end_flag, pad_flag, unk_flag]+char_list # 把特殊标记加入到数组中\n",
    "index_to_char={idx: char for idx, char in enumerate(char_list)} # 建立 index_to_char字典\n",
    "char_to_index={char:idx for idx, char in enumerate(char_list)} # 建立char_to_index字典\n",
    "\n",
    "def text_to_index(texts, char_to_index): # 把数据转化为Index的形式\n",
    "    texts_indexs=[]\n",
    "    for item in texts:\n",
    "        texts_indexs.append([char_to_index.get(char, unk_index) for char in item])\n",
    "    return texts_indexs\n",
    "\n",
    "source_indexs=text_to_index(source_texts, char_to_index) # 原句子转化为Index形式\n",
    "target_indexs=text_to_index(target_texts, char_to_index)   # 目标句子转化为index形式\n",
    "\n",
    "vocab_len=len(index_to_char) # 字典大小(包含多少个字符)\n",
    "source_max_len=max([len(item) for item in source_indexs]) # 原数据句子最大长度\n",
    "target_max_len=max([len(item) for item in target_indexs]) # 目标句子最大长度\n",
    "\n",
    "\n",
    "def pad_sentence_batch(sentence_batch, pad_int): \n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [end_index] + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "def get_batches(batch_size=32):\n",
    "    for batch_i in range(0, len(source_indexs)//batch_size):\n",
    "            start_i = batch_i * batch_size\n",
    "            sources_batch = source_indexs[start_i:start_i + batch_size]\n",
    "            targets_batch  = target_indexs[start_i:start_i + batch_size]\n",
    "            \n",
    "            # 补全序列\n",
    "            pad_sources_batch = np.array(pad_sentence_batch(sources_batch, pad_index))\n",
    "            pad_targets_batch = np.array(pad_sentence_batch(targets_batch, pad_index))\n",
    "            \n",
    "            # 记录每条记录的长度\n",
    "            targets_lengths = []\n",
    "            for target in targets_batch:\n",
    "                targets_lengths.append(len(target)+1)\n",
    "\n",
    "            source_lengths = []\n",
    "            for source in sources_batch:\n",
    "                source_lengths.append(len(source)+1)\n",
    "            yield pad_targets_batch, pad_sources_batch, targets_lengths, source_lengths\n",
    "(valid_targets_batch, valid_sources_batch, valid_targets_lengths, valid_sources_lengths) = next(get_batches()) \n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 00:52:48.345741 4321588096 deprecation.py:323] From /Users/zhouwencheng/Desktop/Grass/02Study/02PythonEnv/envpy3.7/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第几个周期: 1 loss: 0.019352896\n",
      "第几个周期: 2 loss: 0.059628222\n",
      "第几个周期: 3 loss: 0.0008716241\n",
      "第几个周期: 4 loss: 0.0004938206\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "seq2seq=Seq2Seq(source_vocab_size=vocab_len, target_vocab_size=vocab_len)\n",
    "\n",
    "model_save_path=\"/Users/zhouwencheng/Desktop/Grass/data/model/101seq2seqModel/203_s2s_at\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    checkpoint = tf.train.latest_checkpoint(model_save_path)\n",
    "    if checkpoint:\n",
    "        seq2seq.load(sess, checkpoint)\n",
    "    for index in range(1, 5):\n",
    "        for batch_i, (targets_batch, sources_batch, targets_lengths, sources_lengths) in enumerate(\n",
    "                get_batches()):\n",
    "            loss=seq2seq.train(sess, sources_batch, sources_lengths, targets_batch,targets_lengths)\n",
    "        print(\"第几个周期:\", index, \"loss:\", loss) \n",
    "        seq2seq.save(sess, model_save_path+\"/tf_s2s_at_203.ckpt\")\n",
    "        \n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测处理函数\n",
    "\n",
    "def pred_text_to_ids(texts, char_to_index):\n",
    "    unk_index=3\n",
    "    pad_int=2\n",
    "    end_index=1\n",
    "    texts_indexs=[] \n",
    "    for item in texts:\n",
    "        texts_indexs.append([char_to_index.get(char, unk_index) for char in item])\n",
    "    max_sentence = max([len(sentence) for sentence in texts_indexs])\n",
    "    inputs_pad= [sentence + [end_index] + [pad_int] * (max_sentence - len(sentence)) for sentence in texts_indexs]\n",
    "    lengs=[len(item)+1 for item in texts]\n",
    "    max_input_len=max([len(item) for item in text_tests])+1\n",
    "    return inputs_pad, lengs, max_input_len\n",
    "    \n",
    "def index_to_text(ids, index_to_char):\n",
    "    end_index=1\n",
    "    texts=[]\n",
    "    for item in ids:\n",
    "        chars=[]\n",
    "        for index in item:\n",
    "            if index==end_index: \n",
    "                break \n",
    "            chars=chars+[index_to_char.get(index,  '<UNK>')] \n",
    "        texts.append(\"\".join(chars))\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lkjgdaa', 'af']\n",
      "['LKJGDAA', 'AF']\n"
     ]
    }
   ],
   "source": [
    "# 预测 \n",
    "text_tests=[\"lkjgdaa\", \"af\"]\n",
    "inputs_pad, lengs, max_input_len = pred_text_to_ids(text_tests, char_to_index)\n",
    "bath_size=len(text_tests)\n",
    "model_save_path=\"/Users/zhouwencheng/Desktop/Grass/data/model/101seq2seqModel/203_s2s_at\"\n",
    "tf.reset_default_graph()\n",
    "seq2seq=Seq2Seq(source_vocab_size=vocab_len, target_vocab_size=vocab_len, \n",
    "                batch_size=bath_size, max_pred_len = max_input_len,train_mode=False)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    checkpoint = tf.train.latest_checkpoint(model_save_path)\n",
    "    if checkpoint:\n",
    "        seq2seq.load(sess, checkpoint) \n",
    "    pred=seq2seq.predict(sess, inputs_pad, lengs) \n",
    "    \n",
    "pred_text=index_to_text(pred, index_to_char)\n",
    "print(text_tests)\n",
    "print(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lkjgdaaaaaa', 'af']\n",
      "LKJGDAAAA\n",
      "AF\n"
     ]
    }
   ],
   "source": [
    "text_tests=[\"lkjgdaaaaaa\", \"af\"]\n",
    "text_indexs=text_to_index(text_tests, char_to_index)\n",
    "inputs=pad_sentence_batch(text_indexs, pad_index)\n",
    "lengs=[len(item)+1 for item in text_tests]\n",
    "max_input_len=max([len(item) for item in text_tests])+1\n",
    "bath_size=len(text_tests)\n",
    "\n",
    "model_save_path=\"/Users/zhouwencheng/Desktop/Grass/data/model/101seq2seqModel/203_s2s_at\"\n",
    "tf.reset_default_graph()\n",
    "seq2seq=Seq2Seq(source_vocab_size=vocab_len, target_vocab_size=vocab_len, batch_size=bath_size, \n",
    "                max_pred_len = max_input_len,train_mode=False)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    checkpoint = tf.train.latest_checkpoint(model_save_path)\n",
    "    if checkpoint:\n",
    "        seq2seq.load(sess, checkpoint) \n",
    "    pred=seq2seq.predict(sess, inputs, lengs) \n",
    "    \n",
    "print(text_tests)\n",
    "for item in pred:\n",
    "    chars=[]\n",
    "    for index in item:\n",
    "        if index==end_index:\n",
    "            print(\"\".join(chars))\n",
    "            break \n",
    "        chars=chars+[index_to_char.get(index,  '<UNK>')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
