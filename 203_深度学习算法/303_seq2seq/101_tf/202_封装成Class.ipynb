{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(object):\n",
    "    def __init__(self,\n",
    "                 source_vocab_size,\n",
    "                 target_vocab_size,\n",
    "                 target_start_flag_index=0,\n",
    "                 target_end_flag_index=1,\n",
    "                 target_pad_flag_index=2,\n",
    "                 batch_size=32,\n",
    "                 encode_embed_dim=128,\n",
    "                 decode_embed_dim=128,\n",
    "                 rnn_size=128,\n",
    "                 num_layers=2,\n",
    "                 trian_mode=True,\n",
    "                ):\n",
    "        self.source_vocab_size=source_vocab_size\n",
    "        self.target_vocab_size=target_vocab_size\n",
    "        self.target_start_flag_index=target_start_flag_index\n",
    "        self.target_end_flag_index=target_end_flag_index\n",
    "        self.target_pad_flag_index=target_pad_flag_index\n",
    "        self.batch_size=batch_size\n",
    "        self.encode_embed_dim=encode_embed_dim\n",
    "        self.decode_embed_dim=decode_embed_dim\n",
    "        self.rnn_size=rnn_size\n",
    "        self.num_layers=num_layers\n",
    "        self.trian_mode=trian_mode\n",
    "        \n",
    "        self.build_model()\n",
    "\n",
    "    def get_inputs(self): \n",
    "        self.inputs = tf.placeholder(tf.int32, (None, None), name='inputs') # 输入原句 (None, None)\n",
    "        self.targets = tf.placeholder(tf.int32, (None, None), name='targets') # 目标句子 (None, None)\n",
    "        self.learning_rate = tf.placeholder(tf.float32, name='learning_rate') # 学习率 \n",
    "        self.source_sequence_length = tf.placeholder(tf.int32, (None,), name='source_sequence_length') # 原数据长度-(None,) \n",
    "        self.target_sequence_length = tf.placeholder(tf.int32, (None,), name='target_sequence_length') # 目标数据长度 (None, )\n",
    "        self.max_target_sequence_length = tf.reduce_max(self.target_sequence_length, name='max_target_len') # 最大目标长度\n",
    "        \n",
    "\n",
    "    def get_encoder_layer(self, \n",
    "                                input_data,     # 输入tensor   \n",
    "                                source_sequence_length): # 源数据的序列长度  \n",
    "        encoder_embed_input = tf.contrib.layers.embed_sequence(input_data, self.source_vocab_size, self.encode_embed_dim)\n",
    "        def get_lstm_cell(rnn_size):  \n",
    "            lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            return lstm_cell\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell(self.rnn_size) for _ in range(self.num_layers)])\n",
    "        encoder_output, encoder_state = tf.nn.dynamic_rnn(cell, encoder_embed_input, sequence_length=source_sequence_length, dtype=tf.float32)\n",
    "        return encoder_output, encoder_state\n",
    "    \n",
    "    def process_decoder_input(self, data):\n",
    "        ''' 补充start_flag，并移除最后一个字符 ''' \n",
    "        ending = tf.strided_slice(data, [0, 0], [self.batch_size, -1], [1, 1])  # cut掉最后一个字符\n",
    "        decoder_input = tf.concat([tf.fill([self.batch_size, 1], self.target_start_flag_index), ending], 1)\n",
    "        return decoder_input\n",
    "    \n",
    "    def decoding_layer(self,\n",
    "                                source_sequence_length,    # 源数据长度\n",
    "                                target_sequence_length,     # target数据序列长度\n",
    "                                max_target_sequence_length, # target数据序列最大长度\n",
    "                                encoder_state,                   # encoder端编码的状态向量\n",
    "                                decoder_input,                   # decoder端输入\n",
    "                                encoder_outputs,):               # 添加一个注意力机制 \n",
    "                                \n",
    "        # 1. Embedding \n",
    "        decoder_embeddings = tf.Variable(tf.random_uniform([self.target_vocab_size, self.decode_embed_dim]))\n",
    "        decoder_embed_input = tf.nn.embedding_lookup(decoder_embeddings, decoder_input)\n",
    "\n",
    "        # 2. 构造Decoder中的RNN单元\n",
    "        def get_decoder_cell(rnn_size):\n",
    "            decoder_cell = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            return decoder_cell\n",
    "\n",
    "        #2.1 添加注意力机制的RNN 单元\n",
    "        def get_decoder_cell_attention(rnn_size): \n",
    "            attention_states=encoder_outputs \n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(rnn_size, attention_states, memory_sequence_length=source_sequence_length)\n",
    "            decoder_cell = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism, attention_layer_size=rnn_size)\n",
    "            return decoder_cell  \n",
    "        cell = tf.contrib.rnn.MultiRNNCell([get_decoder_cell_attention(self.rnn_size) for _ in range(self.num_layers)])\n",
    "\n",
    "        # 3. Output全连接层\n",
    "        output_layer = Dense(self.target_vocab_size, kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "\n",
    "        # 4. Training decoder\n",
    "        with tf.variable_scope(\"decode\"):\n",
    "            # 得到help对象\n",
    "            training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embed_input,\n",
    "                                                                sequence_length=target_sequence_length,\n",
    "                                                                time_major=False)\n",
    "            # 构造decoder\n",
    "            training_decoder = tf.contrib.seq2seq.BasicDecoder(cell,\n",
    "                                                               training_helper,\n",
    "                                                               initial_state=cell.zero_state(dtype=tf.float32,batch_size=self.batch_size)\n",
    "                                                               ,output_layer=output_layer) \n",
    "\n",
    "            training_decoder_output, _ ,_= tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                                           impute_finished=True,\n",
    "                                                                           maximum_iterations=max_target_sequence_length)\n",
    "        # 5. Predicting decoder\n",
    "        # 与training共享参数\n",
    "        with tf.variable_scope(\"decode\", reuse=True):\n",
    "            # 创建一个常量tensor并复制为batch_size的大小\n",
    "            start_tokens = tf.tile(tf.constant([self.target_start_flag_index], dtype=tf.int32), [self.batch_size], \n",
    "                                   name='start_tokens')\n",
    "            predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings,\n",
    "                                                                    start_tokens,\n",
    "                                                                    self.target_end_flag_index)\n",
    "            predicting_decoder = tf.contrib.seq2seq.BasicDecoder(cell,\n",
    "                                                            predicting_helper,\n",
    "                                                            initial_state=cell.zero_state(dtype=tf.float32,batch_size=self.batch_size)\n",
    "                                                                ,output_layer=output_layer)\n",
    "            predicting_decoder_output, _,_  = tf.contrib.seq2seq.dynamic_decode(predicting_decoder,\n",
    "                                                                impute_finished=True,\n",
    "                                                                maximum_iterations=max_target_sequence_length)\n",
    "        return training_decoder_output, predicting_decoder_output\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.get_inputs() \n",
    "        encoder_outputs, encoder_state = self.get_encoder_layer(input_data=self.inputs, \n",
    "                                                                      source_sequence_length=self.source_sequence_length)\n",
    "        decoder_input = self.process_decoder_input(self.targets) # 预处理后的decoder输入\n",
    "        self.training_decoder_output, self.predicting_decoder_output=self.decoding_layer(\n",
    "            source_sequence_length=self.source_sequence_length,\n",
    "            target_sequence_length=self.target_sequence_length,\n",
    "            encoder_state=encoder_state,\n",
    "            max_target_sequence_length=self.max_target_sequence_length,\n",
    "            decoder_input=decoder_input,\n",
    "            encoder_outputs=encoder_outputs)\n",
    "        self.masks = tf.sequence_mask(self.target_sequence_length, self.max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "        self.loss = tf.contrib.seq2seq.sequence_loss(logits=self.training_decoder_output.rnn_output,\n",
    "                                                             targets=self.targets,\n",
    "                                                             weights=self.masks, \n",
    "                                                             )\n",
    "        self.opt = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        gradients = self.opt.compute_gradients(self.loss)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        self.update = self.opt.apply_gradients(capped_gradients)\n",
    "    \n",
    "    def train(self, \n",
    "               sess, \n",
    "               encoder_inputs, \n",
    "               encoder_inputs_length,\n",
    "               decoder_inputs, \n",
    "               decoder_inputs_length,\n",
    "               learn_rate): \n",
    "        input_feed={\n",
    "            self.inputs.name:encoder_inputs,\n",
    "            self.source_sequence_length.name:encoder_inputs_length,\n",
    "            self.targets.name:decoder_inputs,\n",
    "            self.target_sequence_length.name:decoder_inputs_length,\n",
    "            self.learning_rate.name:learn_rate\n",
    "        }\n",
    "\n",
    "        output_feed = [\n",
    "            self.update, \n",
    "            self.loss ]\n",
    "        _, loss = sess.run(output_feed, input_feed)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  数据处理\n",
    "\n",
    "source_path=\"/Users/zhouwencheng/Desktop/Grass/data/txt/letters/letters_source.txt\"\n",
    "target_path=\"/Users/zhouwencheng/Desktop/Grass/data/txt/letters/letters_up.txt\"\n",
    "\n",
    "# 打开文件\n",
    "with open(source_path, 'r') as f:\n",
    "    source_text=f.read()\n",
    "    source_texts=source_text.split('\\n') # 进行分句后的句子\n",
    "with open(target_path, 'r') as f:\n",
    "    target_text=f.read()\n",
    "    target_texts=target_text.split('\\n')\n",
    "\n",
    "#====== 特殊标记定义 =========#\n",
    "start_flag='<GO>'\n",
    "end_flag='<EOS>'\n",
    "pad_flag='<PAD>'\n",
    "unk_flag='<UNK>'\n",
    "\n",
    "start_index=0\n",
    "end_index=1\n",
    "pad_index=2\n",
    "unk_index=3\n",
    "    \n",
    "all_text=source_text.replace(\"\\n\", \"\")+target_text.replace(\"\\n\", \"\") # 获取全部文本数据\n",
    "char_list=sorted(list(set(all_text))) # 获取字符的数组\n",
    "char_list=[start_flag, end_flag, pad_flag, unk_flag]+char_list # 把特殊标记加入到数组中\n",
    "index_to_char={idx: char for idx, char in enumerate(char_list)} # 建立 index_to_char字典\n",
    "char_to_index={char:idx for idx, char in enumerate(char_list)} # 建立char_to_index字典\n",
    "\n",
    "def text_to_index(texts, char_to_index): # 把数据转化为Index的形式\n",
    "    texts_indexs=[]\n",
    "    for item in texts:\n",
    "        texts_indexs.append([char_to_index.get(char, unk_index) for char in item])\n",
    "    return texts_indexs\n",
    "\n",
    "source_indexs=text_to_index(source_texts, char_to_index) # 原句子转化为Index形式\n",
    "target_indexs=text_to_index(target_texts, char_to_index)   # 目标句子转化为index形式\n",
    "\n",
    "vocab_len=len(index_to_char) # 字典大小(包含多少个字符)\n",
    "source_max_len=max([len(item) for item in source_indexs]) # 原数据句子最大长度\n",
    "target_max_len=max([len(item) for item in target_indexs]) # 目标句子最大长度\n",
    "\n",
    "\n",
    "def pad_sentence_batch(sentence_batch, pad_int): \n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "def get_batches(batch_size=32):\n",
    "    for batch_i in range(0, len(source_indexs)//batch_size):\n",
    "            start_i = batch_i * batch_size\n",
    "            sources_batch = source_indexs[start_i:start_i + batch_size]\n",
    "            targets_batch  = target_indexs[start_i:start_i + batch_size]\n",
    "            \n",
    "            # 补全序列\n",
    "            pad_sources_batch = np.array(pad_sentence_batch(sources_batch, pad_index))\n",
    "            pad_targets_batch = np.array(pad_sentence_batch(targets_batch, pad_index))\n",
    "            \n",
    "            # 记录每条记录的长度\n",
    "            targets_lengths = []\n",
    "            for target in targets_batch:\n",
    "                targets_lengths.append(len(target))\n",
    "\n",
    "            source_lengths = []\n",
    "            for source in sources_batch:\n",
    "                source_lengths.append(len(source))\n",
    "            yield pad_targets_batch, pad_sources_batch, targets_lengths, source_lengths\n",
    "(valid_targets_batch, valid_sources_batch, valid_targets_lengths, valid_sources_lengths) = next(get_batches()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "seq2seq=Seq2Seq(source_vocab_size=93, target_vocab_size=93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.3533422\n",
      "loss: 0.030368052\n",
      "loss: 0.0041936245\n",
      "loss: 0.0029274183\n",
      "loss: 0.0039266176\n",
      "loss: 0.0009129228\n",
      "loss: 0.00044267913\n",
      "loss: 0.00027995108\n",
      "loss: 0.00019522585\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for index in range(1, 10):\n",
    "        for batch_i, (targets_batch, sources_batch, targets_lengths, sources_lengths) in enumerate(\n",
    "                get_batches()):\n",
    "            loss=seq2seq.train(sess, sources_batch, sources_lengths, targets_batch,targets_lengths, 0.001)\n",
    "        print(\"loss:\",loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
