# 词向量

## 词向量之离散表示

- one-hot 编码
- bag of words
- big-gram 和 N-gram
- 共现矩阵



### 离散表示的问题

- 无法衡量向量之间的关系
- 词表维度随着语料库增长膨胀
- n-gram词序列随着语料库膨胀更快
- 数据稀疏问题



## 词向量之连续表示

**分布式假说：** 若果两个词的上下文(context)相同，那么这两个词所表达的语义也是一样的；换言之，两个词的语义是否相同或相识，取决于两个词的上下文内容，上下文内容表示两个词是可以等价替换的。



- NNLM （word to vec 开创篇 2003年）
  - (N-1)个前向量词：one-hot表示
  - 采用线性映射将one-hot表示投影到稠密D维表示
  - 输出层:softmax
  - NNLM的特点
    - 用隐藏层
    - 单向，预测next word
    - 输入层使用
    - 投影层使用的concat

- CBOW（连续词袋）
  - 相对于NNLM的改进
    - 无隐层
    - 使用双向上下文窗口
    - 上下文词序无关
    - 输入层直接使用低稠密表示
    - 投影层简化为求和（平均）
- skip-gram
  - 相对于NNLM的改进
    - 无隐层
    - 输出由softmax来搞定



## word2vec之Hierchical Softmax(分级 softmax)

NNLM和CBOW的softmax计算太大



## Negative sampling





